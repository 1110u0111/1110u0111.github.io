<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="机器学习之K均值、混合高斯模型和期望最大 提纲  聚类与K均值 混合高斯模型与期望最大 期望最大的另一个视角 期望最大的另一个视角 期望最大的理论基础  聚类与K均值 示例 聚类是什么？举个例子，我们想把下图的人物归类，那么我们可以根据衣服样式归类，也可以根据性别、年龄等进行归类。  比如，我们想检测视频中的移动目标，这也是聚类的应用场景。  聚类 由上面的示例，我">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之K均值、混合高斯模型和期望最大">
<meta property="og:url" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/index.html">
<meta property="og:site_name" content="Rebecca的赛博世界">
<meta property="og:description" content="机器学习之K均值、混合高斯模型和期望最大 提纲  聚类与K均值 混合高斯模型与期望最大 期望最大的另一个视角 期望最大的另一个视角 期望最大的理论基础  聚类与K均值 示例 聚类是什么？举个例子，我们想把下图的人物归类，那么我们可以根据衣服样式归类，也可以根据性别、年龄等进行归类。  比如，我们想检测视频中的移动目标，这也是聚类的应用场景。  聚类 由上面的示例，我">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092330178.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092356073.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092634112.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094023139.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094031460.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094102368.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094107927.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210022034.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210028211.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210941474.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224144606.png">
<meta property="og:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224207091.png">
<meta property="article:published_time" content="2022-03-21T02:03:28.000Z">
<meta property="article:modified_time" content="2022-06-02T09:32:58.664Z">
<meta property="article:author" content="Rebecca">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="K均值">
<meta property="article:tag" content="混合高斯模型">
<meta property="article:tag" content="期望最大">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092330178.png">

<link rel="canonical" href="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习之K均值、混合高斯模型和期望最大 | Rebecca的赛博世界</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Rebecca的赛博世界</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习之K均值、混合高斯模型和期望最大
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-21 10:03:28" itemprop="dateCreated datePublished" datetime="2022-03-21T10:03:28+08:00">2022-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-02 17:32:58" itemprop="dateModified" datetime="2022-06-02T17:32:58+08:00">2022-06-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1
id="机器学习之k均值混合高斯模型和期望最大">机器学习之K均值、混合高斯模型和期望最大</h1>
<p>提纲</p>
<ul>
<li>聚类与K均值</li>
<li>混合高斯模型与期望最大</li>
<li>期望最大的另一个视角</li>
<li>期望最大的另一个视角</li>
<li>期望最大的理论基础</li>
</ul>
<h2 id="聚类与k均值">聚类与K均值</h2>
<h3 id="示例">示例</h3>
<p>聚类是什么？举个例子，我们想把下图的人物归类，那么我们可以根据衣服样式归类，也可以根据性别、年龄等进行归类。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092330178.png" class="" title="image-20220321092330178">
<p>比如，我们想检测视频中的移动目标，这也是聚类的应用场景。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092356073.png" class="" title="image-20220321092356073">
<h3 id="聚类">聚类</h3>
<p>由上面的示例，我们对聚类应该有了一个大致的意识。聚类的基本思想就是将相似的实例分组在一起。什么才叫做相似的实例？如果两个点之间的距离足够近，那我们就认为他们是相似的，就像下图这种2D点的模式。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092634112.png" class="" title="image-20220321092634112">
<p>两个点之间距离的衡量有很多种方法，在这篇文章中，我们取欧式距离，公式如下：
<span class="math display">\[
dist(\boldsymbol{x},\boldsymbol{x})=||\boldsymbol{x}-\boldsymbol{y}||_2^2
\]</span> 其中，<span class="math inline">\(\boldsymbol{x}\)</span>
是一个向量，<span class="math inline">\(||\cdot||_2\)</span>
是两个向量之间的二范数，其计算方法为 <span
class="math inline">\(||\boldsymbol{x}||_2=(\sum \limits_{i=1}^n x_i^2)^
\frac{1}{2}\)</span> 。</p>
<p>我们的聚类结果取决于簇内相似度和簇间相似度，一般来说，我们希望簇内相似度更高，簇间相似度更低。</p>
<h3 id="聚类方法">聚类方法</h3>
<ul>
<li>原型聚类——K均值算法、高斯混合聚类</li>
<li>密度聚类——DBSCAN算法、Mean-Shift算法</li>
<li>层次聚类——–Agglomerative 算法、Divisive算法、BIRCH 算法</li>
<li>谱聚类</li>
</ul>
<h3 id="聚类例子">聚类例子</h3>
<p>我们举几个聚类所应用的场景。</p>
<ul>
<li><p>图像分割，目标：将图像分割成有意义的或感知上相似的区域</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094023139.png" class="" title="image-20220321094023139"></li>
<li><p>基因表达数据聚类</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094031460.png" class="" title="image-20220321094031460">
<p>[^]: Eisen et al,PNAS 1998</p></li>
<li><p>聚类新闻文章</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094102368.png" class="" title="image-20220321094102368"></li>
<li><p>进化树</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094107927.png" class="" title="image-20220321094107927">
<p>[^]: Lindbald-Toh et al., Nature 2005</p></li>
</ul>
<h3 id="k均值聚类">K均值聚类</h3>
<h4 id="准备">准备</h4>
<p>在 <span class="math inline">\(D\)</span> 维的欧式空间里，给定数据集
<span class="math inline">\({x_1,x_2,...,x_n}\)</span> ，将其划分为
<span class="math inline">\(K\)</span> 个簇（类），这是给定的<span
class="math inline">\(K\)</span> 个编码之一。同时引入指标变量 <span
class="math inline">\(r_{nk} \in \lbrace 0,1
\rbrace\)</span>，表示数据点 <span class="math inline">\(x_n\)</span>
属于K个聚类中的哪一个。</p>
<h4 id="失真度量">失真度量</h4>
<p>K均值算法针对聚类所得簇 <span class="math inline">\(C=\lbrace
C_1,C_2,...,C_K \rbrace\)</span> ，最小化平方误差和： <span
class="math display">\[
J=\sum \limits_{n=1}^N \sum \limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2
\]</span> 其中，<span class="math inline">\(\mu_k = \frac{1}{|C_k|} \sum
\limits_{x \in C_k}x\)</span> ，是簇 <span
class="math inline">\(C_k\)</span> 的均值（<span class="math inline">\(k
= 1,2,...,K\)</span>）。</p>
<p>我们的目标其实就是找到 <span class="math inline">\(\lbrace r_{nk}
\rbrace\)</span> 和 <span class="math inline">\(\lbrace \mu_k
\rbrace\)</span> 的值，使得 <span class="math inline">\(J\)</span>
达到最小值。也就是说，我们希望找到对于每个簇 <span
class="math inline">\(C_k\)</span>
，找到它的指标变量和均值，使得簇内每个点的距离最小。直观来看，这个式子在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，<span
class="math inline">\(J\)</span> 值越小，簇内样本相似度越高。</p>
<p>我们可以⽤⼀种迭代的⽅法完成这件事，其中每次迭代涉及到两个连续的步骤，分别关于<span
class="math inline">\(\lbrace r_{nk} \rbrace\)</span> 的最优化和 <span
class="math inline">\(\lbrace \mu_k \rbrace\)</span> 的最优化。</p>
<h4
id="寻找lbrace-r_nk-rbrace-的最优化和-lbrace-mu_k-rbrace-的最优化">寻找<span
class="math inline">\(\lbrace r_{nk} \rbrace\)</span> 的最优化和 <span
class="math inline">\(\lbrace \mu_k \rbrace\)</span> 的最优化</h4>
<ol type="1">
<li>首先选择 <span class="math inline">\(\lbrace \mu_k \rbrace\)</span>
的初始值；</li>
<li>第一阶段：我们关于 <span class="math inline">\(r_{nk}\)</span>
最小化 <span class="math inline">\(J\)</span>，保持 <span
class="math inline">\(\mu_k\)</span> 固定；</li>
<li>第二阶段：我们关于 <span class="math inline">\(\mu_k\)</span> 最小化
<span class="math inline">\(J\)</span>，保持 <span
class="math inline">\(r_{nk}\)</span> 固定；</li>
</ol>
<p>注：两个阶段分别对应于EM算法中的E（期望）步骤和M（最⼤化）步骤，EM算法稍后会介绍，这里只是引入这样的一个概念。</p>
<h4 id="e参数-r_nk-的选择">E：参数 <span
class="math inline">\(r_{nk}\)</span> 的选择</h4>
<ol type="1">
<li><p>由于平方误差和 <span class="math inline">\(J=\sum \limits_{n=1}^N
\sum \limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2\)</span> 是一个关于 <span
class="math inline">\(r_{nk}\)</span> 的线性函数（这里再次提醒，<span
class="math inline">\(r_{nk} \in \lbrace 0,1
\rbrace\)</span>，表示数据点 <span class="math inline">\(x_n\)</span>
属于 <span class="math inline">\(K\)</span>
个聚类中的哪一个），因此，这个优化问题存在解析解。</p></li>
<li><p>而显然，<span class="math inline">\(J=\sum \limits_{n=1}^N \sum
\limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2 = \sum \limits_{k=1}^K
r_{1k}||x_1-\mu_k||^2+...+ \sum \limits_{k=1}^K
r_{Nk}||x_N-\mu_k||^2\)</span> ，其中有不同的 <span
class="math inline">\(n\)</span> 的项是独立的，因此，我们可以对每个
<span class="math inline">\(n\)</span> 分别进行最优化，只要 <span
class="math inline">\(k\)</span> 的值使得 <span
class="math inline">\(||x_n-\mu_k||^2\)</span> 最小，那么就使 <span
class="math inline">\(r_{nk}=1\)</span>。其实就是我们对于每一个数据点
<span class="math inline">\(x_n\)</span> ，都计算 <span
class="math inline">\(x_n\)</span> 与每个簇 <span
class="math inline">\(C_k\)</span> 的中心点（即 <span
class="math inline">\(\mu_k\)</span>）之间的距离，通过距离的比较，找到与
<span class="math inline">\(x_n\)</span> 最近的簇，将 <span
class="math inline">\(r_{nk}\)</span> 置为1，表示这个数据点 <span
class="math inline">\(x_n\)</span> 已经被分到簇 <span
class="math inline">\(C_k\)</span> 中了。</p></li>
<li><p>因此，我们可以对参数 <span class="math inline">\(r_{nk}\)</span>
选择如下： <span class="math display">\[
   r_{nk} =
   \begin{cases}
   1,  &amp; \text{if} \ k= \text{argmin}_j||x_n-\mu_j||^2 \\
   0, &amp; \text{otherwise}
   \end{cases}
   \]</span></p></li>
<li><p>直观地说，其实就是将 <span class="math inline">\(x_n\)</span>
分到最近的一个类里面。</p></li>
</ol>
<h4 id="m优化-mu_k">M：优化 <span
class="math inline">\(\mu_k\)</span></h4>
<ol type="1">
<li><p>固定 <span class="math inline">\(r_{nk}\)</span> ；</p></li>
<li><p>由于平方误差和 <span class="math inline">\(J=\sum \limits_{n=1}^N
\sum \limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2\)</span> 是一个关于 <span
class="math inline">\(\mu_k\)</span> 的线性函数，（这里提醒一句，<span
class="math inline">\(\mu_k = \frac{1}{|C_k|} \sum \limits_{x \in
C_k}x\)</span> ，是簇 <span class="math inline">\(C_k\)</span>
的均值（<span class="math inline">\(k =
1,2,...,K\)</span>）因此，我们只要令 <span
class="math inline">\(J\)</span> 关于 <span
class="math inline">\(\mu_k\)</span> 的导数等于零，<span
class="math inline">\(J\)</span> 即可达到最小值。</p></li>
<li><p>求解过程为： <span class="math display">\[
2\sum \limits_{n=1}^N r_{nk}(x_n-\mu_k)=0
\]</span> 解出结果为 <span class="math display">\[
\mu_k = \frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}
\]</span></p></li>
<li><p>直观地说，其实就是令 <span class="math inline">\(\mu_k\)</span>
等于类别为 <span class="math inline">\(k\)</span>
的所有数据点的均值，<span class="math inline">\(\sum_n r_{nk}\)</span>
为类别为 <span class="math inline">\(k\)</span>
的所有数据点的数量，<span class="math inline">\(\sum_n
r_{nk}x_n\)</span> 等于类别为 <span class="math inline">\(k\)</span>
的所有数据点的总和。</p></li>
</ol>
<h4 id="k均值算法终止">K均值算法终止</h4>
<p>K均值算法，就是重复执行上述的选择 <span
class="math inline">\(r_{nk}\)</span> 和优化 <span
class="math inline">\(\mu_k\)</span>
的步骤，即重复分配数据点给簇（或类）和重复计算簇（或类）中心，直到聚类的分配不改变。由于每个阶段都减小了目标函数
<span class="math inline">\(J\)</span>
的值，因此算法的收敛性得到了保证。</p>
<h4 id="示例来自prml">示例（来自PRML）</h4>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210022034.png" class="" title="image-20220321210022034">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210028211.png" class="" title="image-20220321210028211">
<ul>
<li><span class="math inline">\(J\)</span>
是老忠实间歇喷泉数据的代价函数。</li>
<li>我们故意将聚类中⼼选择了较差的初始值，从⽽算法在收敛之前执⾏了若⼲步。更好的选择是将聚类中⼼选择为由k个随机数据点组成的⼦集。</li>
<li>K均值算法本身经常被用于在EM算法之前初始化高斯混合模型的参数。（这里出现了一个新概念：混合高斯模型，什么是混合高斯模型，这就是接下来要介绍的。）</li>
</ul>
<h3 id="局部最小值">局部最小值</h3>
<p>K均值算法的目标函数 <span class="math inline">\(J=\sum
\limits_{n=1}^N \sum \limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2\)</span>
是非凸的，因此，并不能保证 <span class="math inline">\(J\)</span>
上的坐标下降收敛到全局最小值，没有什么办法能够阻止K均值陷入局部极小值。我们可以尝试更多随机初始化的点，或者尝试非局部拆分和合并操作，非局部拆分就是同时合并相邻的两个簇，合并操作就是将一个大的簇分为两个。</p>
<h3 id="k均值算法的性质">K均值算法的性质</h3>
<ol type="1">
<li>保证在有限次迭代中收敛。</li>
<li>每次迭代的训练时间：将数据点分配到最近的簇中心是 <span
class="math inline">\(O(KN)\)</span> 时间，根据分配的点改变聚类中心是
<span class="math inline">\(O(N)\)</span> 时间。</li>
</ol>
<h3 id="k均值的局限">K均值的局限</h3>
<ul>
<li>在每⼀次迭代中，每个数据点被分配到⼀个唯⼀的聚类中（“硬”分配）<br />
</li>
<li>某些点可能到两个聚类中心的距离相等</li>
<li>通过使⽤概率的⽅法，我们得到对数据点聚类的“软”分配，它反映出在最合适聚类分配上的不确定性</li>
</ul>
<h4 id="图像分割与压缩">图像分割与压缩</h4>
<p>之前提到，聚类算法其实可以用于图像分割与压缩。我们的目标是，将图像分割成若干的区域，每个区域有⼀个相对相似的视觉外观，或者对应于某个物体，或物体的⼀部分。在计算机图像中，每个像素是⼀个3维空间中的⼀个点，三维空间由{红、绿、蓝}
通道的三个亮度值构成，每个值由8比特的精度存储。</p>
<p>K均值聚类与K个颜色的调色板一起使用，该方法没有考虑不同像素的空间上的近似性。</p>
<h5 id="图像分割">（1）图像分割</h5>
<p>以下是选择2、3和10种颜色对彩色图像进行编码的两个示例：</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210941474.png" class="" title="image-20220321210941474">
<p>（2）数据压缩</p>
<ul>
<li><p>无损数据压缩：能够从压缩的表⽰中精确地重建原始数据。</p></li>
<li><p>有损数据压缩：接受重建过程中出现的⼀些错误。</p></li>
<li><p>K-Means应用于有损数据压缩</p>
<ul>
<li>对于 $ N $ 个数据点中的每一个，我们只存储它被分配的聚类种类 <span
class="math inline">\(k\)</span>。</li>
<li>我们还存储了 <span class="math inline">\(K\)</span>个聚类中心 <span
class="math inline">\(μ_k\)</span>
的值，这通常需要存储小得多的数据,其中我们假定 <span
class="math inline">\(K ≪ N\)</span>。这样，每个数据点都根据它最近的中心
<span class="math inline">\(μ_k\)</span>
确定。新的数据点可以类似地压缩。</li>
<li>首先找到最近的 <span
class="math inline">\(μ_k\)</span>，然后存储标签 <span
class="math inline">\(k\)</span> 而不是原始的数据向量</li>
</ul>
<p>这个框架被称为向量量子化(vector quantization)，向量 <span
class="math inline">\(μ_k\)</span> 被称为编码书向量 (code-book
vector)。</p></li>
<li><p>上面讨论的图像分割问题也说明了数据压缩中聚类的使用。</p>
<ul>
<li>假设原始图像有 $ N $ 个像素，每个像素由$ {R, G, B} <span
class="math inline">\(三个值组成，每个值由\)</span> 8 <span
class="math inline">\(比特的精度存储。这样，直接传递整幅图像需要\)</span>
24N $比特。</li>
<li>现在假设我们首先在图像数据上运行K均值算法，然后，我们不直接传递原始像素亮度向量，而是传递最近的向量$
_k <span class="math inline">\(的亮度。由于有\)</span> K <span
class="math inline">\(个这样的向量，因此每个像素需要\)</span> _2K
$比特。</li>
<li>我们还必须传送$ K <span class="math inline">\(个编码书向量\)</span>
_k <span class="math inline">\(，这需要\)</span> 24K $比特</li>
<li>因此传递这个图像所需的比特总数为$ 24K + N _2K
$（四舍五入到最近的整数）</li>
<li>当 <span class="math inline">\(K=2,3\)</span> 以及 <span
class="math inline">\(10\)</span>
时，压缩为原来的比例分别为4%，8%，17%</li>
</ul></li>
</ul>
<h2 id="混合高斯模型与期望最大">混合高斯模型与期望最大</h2>
<h3 id="单高斯模型">单高斯模型</h3>
<p>一维高斯（正态）分布的概率密度函数如下： <span
class="math display">\[
f(x)=\frac{1}{\sqrt{2 \pi} \sigma}
\text{exp}(-\frac{(x-\mu)^2}{2\sigma^2})
\]</span> 其中，<span class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma^2\)</span> 分别是高斯分布的均值和方差。</p>
<h3 id="二维高斯模型">二维高斯模型</h3>
<p>多维变量 <span class="math inline">\(X=\lbrace x_1,x_2,...,x_n
\rbrace\)</span> 的联合概率密度函数为： <span class="math display">\[
f(X)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(X-\mu)^T
\Sigma ^{-1}(X-\mu)],X=(x_1,x_2,...,x_n)
\]</span> 其中，<span class="math inline">\(d\)</span>
为变量维度，对于二维高斯分布，有 <span
class="math inline">\(d=2\)</span> ；<span class="math inline">\(u =
\begin{pmatrix} u_1 \\ u_2 \\ ...\\ u_n\end{pmatrix}\)</span>
，为各维变量的均值，<span class="math inline">\(\Sigma\)</span>
为协方差矩阵，描述各维变量之间的相关度，对于二维高斯分布，有 <span
class="math display">\[
\Sigma = \begin{bmatrix} \delta_{11} \ \delta_{12} \\ \delta_{21} \
\delta_{22}\end{bmatrix}
\]</span></p>
<h3 id="高斯混合模型-gmm">高斯混合模型 (GMM)</h3>
<p>一个高斯混合模型能表示成如下分布： <span class="math display">\[
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span> 其中 <span
class="math inline">\(\mathcal{N}(x|\mu_k,\Sigma_k)\)</span>
就是上面的多维变量的高斯模型。而混合系数 <span
class="math inline">\(\pi_k\)</span> 满足 <span class="math inline">\(0
\le \pi_k \le 1\)</span> 和 <span class="math inline">\(\sum
\limits_{k=1}^K \pi_k = 1\)</span> 。</p>
<p>某个混合高斯模型融合的高斯模型个数足够多，它们之间的权重设定得足够合理，这个混合模型可以拟合任意分布的样本。</p>
<h3 id="一维高斯混合函数的边缘分布">一维高斯混合函数的边缘分布</h3>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224144606.png" class="" title="image-20220328224144606">
<p>⾼斯混合概率分布可以写成高斯分布的线性叠加的形式</p>
<h3 id="二维高斯混合函数的边缘分布">二维高斯混合函数的边缘分布</h3>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224207091.png" class="" title="image-20220328224207091">
<h3 id="gmms-和隐变量">GMMs 和隐变量</h3>
<p>我们引入离散潜在变量 <span class="math inline">\(z\)</span>
来描述高斯混合模型，更加深刻地认识这个重要的分布，开始了解期望最大化算法。</p>
<p>首先，我们已经知道，<span class="math inline">\(K\)</span>
个高斯分布的线性叠加 <span class="math display">\[
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span> 然后，我们可以引入一个 <span class="math inline">\(K\)</span>
维的二维随机变量 <span class="math inline">\(z\)</span> ，用 1-of-K
表示（one-hot 变量），即 <span class="math inline">\(z = \lbrace
z_1,z_2,...,z_k \rbrace\)</span> ，其中 <span class="math inline">\(z_k
\in \lbrace 0,1 \rbrace\)</span> 且 <span class="math inline">\(\sum_k
z_k=1\)</span> ，向量 <span class="math inline">\(z\)</span> 有 <span
class="math inline">\(K\)</span> 种可能的状态。</p>
<p>那么，我们就可以确定边缘概率分布 <span
class="math inline">\(p(z)\)</span> 了。</p>
<h3 id="确定边缘概率分布-pz">确定边缘概率分布 <span
class="math inline">\(p(z)\)</span></h3>
<p>我们将概率与每个组件 <span class="math inline">\(z_k\)</span>
相关联。那么它们的关系要怎么连接呢？将 <span
class="math inline">\(z\)</span> 的边缘概率分布根据混合系数<span
class="math inline">\(π_k\)</span> 进⾏赋值，即 <span
class="math display">\[
p(z_k = 1)=\pi_k
\]</span> 其中 <span class="math inline">\(k\)</span> 满足 <span
class="math inline">\(0 \le \pi_k \le 1\)</span> 且 <span
class="math inline">\(\sum_k \pi_k = 1\)</span> 。</p>
<p>由于 <span class="math inline">\(z\)</span> 使用了“ 1-of-K ”
的表示方法，因此 <span class="math inline">\(z\)</span>
的边缘概率分布也可以写成 <span class="math display">\[
p(z)=\prod \limits_{k=1}^K \pi_k^{z_k}
\]</span> 当有1个分量时，<span class="math inline">\(p(z_1) =
\pi_k^{z_1}\)</span> ；当有2个分量时，<span
class="math inline">\(p(z_1,z_2)=\pi_1^{z_1}\pi_2^{z_2}\)</span> 。</p>
<h3 id="确定条件概率-pxz">确定条件概率 $p(x|z) $</h3>
<p>给定 <span class="math inline">\(z\)</span> 的⼀个特定的值，<span
class="math inline">\(x\)</span> 的条件概率分布是⼀个高斯分布 <span
class="math display">\[
p(x|z_k=1) = \mathcal{N}(x|\mu_k,\Sigma_k)
\]</span> 因此，条件概率 $p(x|z) $可以写成如下形式： <span
class="math display">\[
p(x|z) = \prod \limits_{k=1}^K \mathcal{N}(x|\mu_k,\Sigma_k)^{z_k}
\]</span> 由于指数 <span class="math inline">\(z_k\)</span>
的存在，这个连乘中除了一个乘积项之外其余的都等于1.</p>
<h3 id="联合分布">联合分布</h3>
<p>定义隐变量 <span class="math inline">\(z\)</span> 和观测变量 <span
class="math inline">\(x\)</span> 的联合分布 <span
class="math display">\[
p(x, z)=p(z)p(x|z)
\]</span> 其中，<span class="math inline">\(x\)</span> 为观测变量，<span
class="math inline">\(z\)</span> 为隐变量，<span
class="math inline">\(p(z)\)</span> 为边缘分布，<span
class="math inline">\(p(x|z)\)</span>为条件分布。</p>
<h3 id="边缘概率分布px">边缘概率分布p(x)</h3>
<p>由于联合分布是 <span class="math inline">\(p(x,
z)=p(z)p(x|z)\)</span> ，因此，<span class="math inline">\(x\)</span>
的边缘分布可以通过将联合概率分布对所有可能的 <span
class="math inline">\(z\)</span> 求和的方式得到，即 <span
class="math display">\[
\begin{aligned}
p(x) &amp; = \sum\limits_zp(z)p(x|z) \\
&amp; = \sum\limits_z(\prod \limits_{k=1}^K \pi_k^{z_k}
\mathcal{N}(x|\mu_k,\Sigma_k)^{z_k}) \\
&amp; = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\end{aligned}
\]</span> <span class="math inline">\(x\)</span>
的分布是高斯混合模型的标准形式。</p>
<p>对联合概率分布 <span class="math inline">\(p(x, z)\)</span>
操作，⽽不是对边缘概率分布 <span class="math inline">\(p(x)\)</span>
操作，这会产⽣极⼤的计算上的简化。</p>
<h3 id="另一个条件概率-responsibilitypzx">另一个条件概率
(Responsibility)<span class="math inline">\(p(z|x)\)</span></h3>
<p>另一个起着重要作用的量是给定$ x <span
class="math inline">\(的条件下，\)</span> z <span
class="math inline">\(的条件概率。我们会用\)</span> (z_k) <span
class="math inline">\(表示\)</span> p(z_k=1|x)
$，它的值可以使用贝叶斯定理求出 $ <span
class="math display">\[\begin{eqnarray} \gamma(z_k) \equiv p(z_k = 1|x)
&amp; = &amp; \frac{p(z_k = 1)p(x|z_k =
1)}{\sum\limits_{j=1}^Kp(z_j=1)p(x|z_j = 1)} \ &amp; = &amp;
\frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum\limits_{j=1}^K\pi_j\mathcal{N}(x|\mu_j,\Sigma_k)}\end{eqnarray}\]</span>
$ 我们将 $ _k $ 看成 $ z_k = 1 $ 的先验概率，将 $ (z_k) $ 看成观测到$ x
<span
class="math inline">\(之后，对应的后验概率。正如我们将看到的那样，\)</span>
(z_k) $ 也可以被看做分量 $ k $ 对于“解释”观测值 <span
class="math inline">\(x\)</span> 的“责任”（responsibility）。</p>
<h3 id="gmm的最大似然">GMM的最大似然</h3>
<p>假设我们有⼀个观测的数据集 <span
class="math inline">\(x_1,x_2,...,x_N\)</span>
，我们希望使用混合高斯模型来对数据进行建模（ <span
class="math inline">\(N\)</span> 条数据，维度都为 <span
class="math inline">\(D\)</span> ），则数据集可以表示成 <span
class="math inline">\(N×D\)</span> 的矩阵 <span class="math inline">\(X
= \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \\ ...
\\\mathbf{x}_N\end{bmatrix}\)</span> ， <span
class="math inline">\(\mathbf{x}_n = [\mathbf{x}_{n1} \ \mathbf{x}_{n2}
\ ...\  \mathbf{x}_{nD}]\)</span> 。对应隐含变量会被表示为⼀个 <span
class="math inline">\(N×K\)</span> 的矩阵 <span class="math inline">\(Z
= \begin{bmatrix} \mathbf{z}_1 \\ \mathbf{z}_2 \\ ...
\\\mathbf{z}_N\end{bmatrix}\)</span>， <span
class="math inline">\(\mathbf{z}_n = [\mathbf{z}_{n1} \ \mathbf{z}_{n2}
\ ...\  \mathbf{z}_{nK}]\)</span>。</p>
<p>目标是表示这个似然函数，从而通过最大化似然函数来估计这三组参数 <span
class="math inline">\(\pi_k,\mu_k,\Sigma_k\)</span></p>
<p>混合密度函数为 <span class="math display">\[
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span> 因此，对数似然函数为 <span class="math display">\[
\ln p(\boldsymbol{X} \mid \boldsymbol{\pi}, \boldsymbol{\mu},
\boldsymbol{\Sigma})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k}
\mathcal{N}\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k},
\boldsymbol{\Sigma}_{k}\right)\right\}
\]</span> 这是一个比单一高斯更难的问题。</p>
<h3 id="最大化对数似然函数">最大化对数似然函数</h3>
<p>目标是估计以下三个参数集 <span
class="math inline">\(\pi_k,\mu_k,\Sigma_k\)</span>​
，Ø在保持其他导数不变的情况下，依次求导数，但是没有解析解。这项任务并不简单，因为对k的求和出现在对数计算内部，从而对数函数不再直接作用于高斯分布。虽然基于梯度的优化是可能的，我们考虑更一般的迭代EM算法。</p>
<h3 id="gmm-最大似然估计的一些问题">GMM 最大似然估计的一些问题</h3>
<p>在最大化似然函数之前，简单地提两个技术问题：</p>
<p>• 病态解，即高斯混合的奇异性问题。</p>
<p>• 混合的可区分问题</p>
<h4 id="病态解">病态解</h4>
<p>我们考虑一个高斯混合模型，它的分量的协方差矩阵为 $ _k = _k^2I <span
class="math inline">\(，其中\)</span> I
$是一个单位矩阵，结论对于一般的协方差矩阵仍然成立。假设混合模型的第 $ j
$ 个分量的均值 $ _j $ 与某个数据点完全相同，即对于某个 $ n $ 值，$ _j =
x_n $ 。这样，这个数据点会为似然函数贡献一项，形式为</p>
<p><span class="math display">\[
\mathcal{N}(x_n|x_n,\sigma_j^2I) =
\frac{1}{(2\pi)^{D/2}}\frac{1}{\sigma_j^D}
\]</span> 如果我们考虑极限$ _j
$，那么我们看到这一项趋于无穷大，因此对数似然函数也会趋于无穷大。</p>
<p>回忆一下，这个问题在单一的高斯分布中没有出现。为了理解不同之处，我们注意到，如果单一的高斯分布退化到了一个数据点上，那么它总会给由其他数据点产生的似然函数贡献可乘的因子，这些因子会以指数的速度趋于0，从而使得整体的似然函数趋于零而不是无穷大。</p>
<p>使⽤合适的启发式⽅法可以避免这个问题：重置均值或协方差。</p>
<h4 id="可区分问题">可区分问题</h4>
<p>对于任意给定的最大似然解，一个由 <span
class="math inline">\(K\)</span> 个分量混合而成的概率分布总共会有 <span
class="math inline">\(K!\)</span> 个等价的解，对应于 <span
class="math inline">\(K!\)</span> 种将 <span
class="math inline">\(K\)</span> 个参数集合分配到 <span
class="math inline">\(K\)</span>
个分量上的方式。换句话说，对于参数值空间中任意给定的点，都会有 <span
class="math inline">\(K!−1\)</span>
个其他的点给出完全相同的概率分布。这个问题被称为可区分（identifiability）问题。但是，这个问题与找到一个好的概率模型无关，因为任意等价的解互相之间都一样好。</p>
<h3 id="用于高斯混合模型的em">用于高斯混合模型的EM</h3>
<p>终于突入这个问题了！</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Rebecca
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/" title="机器学习之K均值、混合高斯模型和期望最大">http://example.com/2022/03/21/机器学习之K均值、混合高斯模型和期望最大/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/K%E5%9D%87%E5%80%BC/" rel="tag"># K均值</a>
              <a href="/tags/%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/" rel="tag"># 混合高斯模型</a>
              <a href="/tags/%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/" rel="tag"># 期望最大</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="prev" title="机器学习之线性回归">
      <i class="fa fa-chevron-left"></i> 机器学习之线性回归
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/" rel="next" title="">
       <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E5%9D%87%E5%80%BC%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7"><span class="nav-number">1.</span> <span class="nav-text">机器学习之K均值、混合高斯模型和期望最大</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E4%B8%8Ek%E5%9D%87%E5%80%BC"><span class="nav-number">1.1.</span> <span class="nav-text">聚类与K均值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.1.1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">1.1.2.</span> <span class="nav-text">聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.3.</span> <span class="nav-text">聚类方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E4%BE%8B%E5%AD%90"><span class="nav-number">1.1.4.</span> <span class="nav-text">聚类例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="nav-number">1.1.5.</span> <span class="nav-text">K均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%86%E5%A4%87"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%B1%E7%9C%9F%E5%BA%A6%E9%87%8F"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">失真度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%BB%E6%89%BElbrace-r_nk-rbrace-%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96%E5%92%8C-lbrace-mu_k-rbrace-%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">寻找\(\lbrace r_{nk} \rbrace\) 的最优化和 \(\lbrace \mu_k \rbrace\) 的最优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#e%E5%8F%82%E6%95%B0-r_nk-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.5.4.</span> <span class="nav-text">E：参数 \(r_{nk}\) 的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#m%E4%BC%98%E5%8C%96-mu_k"><span class="nav-number">1.1.5.5.</span> <span class="nav-text">M：优化 \(\mu_k\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E7%BB%88%E6%AD%A2"><span class="nav-number">1.1.5.6.</span> <span class="nav-text">K均值算法终止</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E6%9D%A5%E8%87%AAprml"><span class="nav-number">1.1.5.7.</span> <span class="nav-text">示例（来自PRML）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC"><span class="nav-number">1.1.6.</span> <span class="nav-text">局部最小值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-number">1.1.7.</span> <span class="nav-text">K均值算法的性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E7%9A%84%E5%B1%80%E9%99%90"><span class="nav-number">1.1.8.</span> <span class="nav-text">K均值的局限</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%8E%E5%8E%8B%E7%BC%A9"><span class="nav-number">1.1.8.1.</span> <span class="nav-text">图像分割与压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="nav-number">1.1.8.1.1.</span> <span class="nav-text">（1）图像分割</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7"><span class="nav-number">1.2.</span> <span class="nav-text">混合高斯模型与期望最大</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.1.</span> <span class="nav-text">单高斯模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.2.</span> <span class="nav-text">二维高斯模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B-gmm"><span class="nav-number">1.2.3.</span> <span class="nav-text">高斯混合模型 (GMM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E5%87%BD%E6%95%B0%E7%9A%84%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83"><span class="nav-number">1.2.4.</span> <span class="nav-text">一维高斯混合函数的边缘分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E5%87%BD%E6%95%B0%E7%9A%84%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83"><span class="nav-number">1.2.5.</span> <span class="nav-text">二维高斯混合函数的边缘分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gmms-%E5%92%8C%E9%9A%90%E5%8F%98%E9%87%8F"><span class="nav-number">1.2.6.</span> <span class="nav-text">GMMs 和隐变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9A%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83-pz"><span class="nav-number">1.2.7.</span> <span class="nav-text">确定边缘概率分布 \(p(z)\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9A%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-pxz"><span class="nav-number">1.2.8.</span> <span class="nav-text">确定条件概率 $p(x|z) $</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E5%88%86%E5%B8%83"><span class="nav-number">1.2.9.</span> <span class="nav-text">联合分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83px"><span class="nav-number">1.2.10.</span> <span class="nav-text">边缘概率分布p(x)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E4%B8%AA%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-responsibilitypzx"><span class="nav-number">1.2.11.</span> <span class="nav-text">另一个条件概率
(Responsibility)\(p(z|x)\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gmm%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6"><span class="nav-number">1.2.12.</span> <span class="nav-text">GMM的最大似然</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.13.</span> <span class="nav-text">最大化对数似然函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gmm-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.14.</span> <span class="nav-text">GMM 最大似然估计的一些问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%97%85%E6%80%81%E8%A7%A3"><span class="nav-number">1.2.14.1.</span> <span class="nav-text">病态解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E5%8C%BA%E5%88%86%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.14.2.</span> <span class="nav-text">可区分问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84em"><span class="nav-number">1.2.15.</span> <span class="nav-text">用于高斯混合模型的EM</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rebecca"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Rebecca</p>
  <div class="site-description" itemprop="description">弱小和无知不是生存的障碍，傲慢才是。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022-03 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rebecca</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
