<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="弱小和无知不是生存的障碍，傲慢才是。">
<meta property="og:type" content="website">
<meta property="og:title" content="Rebecca的赛博世界">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Rebecca的赛博世界">
<meta property="og:description" content="弱小和无知不是生存的障碍，傲慢才是。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Rebecca">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Rebecca的赛博世界</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Rebecca的赛博世界</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/04/%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/04/%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">机器学习之卷积神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-06-04 00:00:14 / 修改时间：00:38:01" itemprop="dateCreated datePublished" datetime="2022-06-04T00:00:14+08:00">2022-06-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Convolutional-Neural-Network-CNN"><a href="#Convolutional-Neural-Network-CNN" class="headerlink" title="Convolutional Neural Network(CNN)"></a>Convolutional Neural Network(CNN)</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>本文章主要记录李宏毅老师在2021-2022年关于卷积神经网络的教学内容，具体可以查看B站的课程，链接为：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=31">https://www.bilibili.com/video/BV1Wv411h7kN?p=31</a>.</p>
<p>本节主要内容讲述的是神经网络架构的设计和思路。</p>
<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>我们引入一个例子：图像分类（Image Classification），也就是说，给一张图片给机器，机器需要去决定这张图片里面有什么样的东西。</p>
<ol>
<li><p>输入</p>
<p>在下面的讨论中，我们假设我们的模型输入的图片大小是固定的，比如 $100 \times 100$。</p>
<p>当然，实际生活中，我们遇到的图像其实大小是不一的。遇到这种情况时，现在比较流行的做法还是把所有图片都先<strong>重新调整（Rescale）</strong>成大小一样再输入。</p>
</li>
<li><p>输出</p>
<p>我们模型的目标是分类，因此，我们会把每一个类别，表示成一个<strong>独热编码（One-hot）</strong>的<strong>向量（Vector）$\hat{y}$</strong> 。如下图所示，如果一张图片对应的是猫的话，那么“猫”所对应的<strong>维度（dimension）</strong>的数值就是1，其余的维度（dimension）的数值就是0。那么这个维度的长度，就代表着你现在的模型可以辨识出多少不同种类的东西。</p>
</li>
</ol>
<ol>
<li><p>目标</p>
<p>模型的输出经过Softmax函数之后，我们可以得到一个向量 $y’$ 。我们希望 $y’$ 和 $\hat{y}$ 的交叉熵（Cross Entropy）越小越好。</p>
</li>
<li><p>问题</p>
<ol>
<li><p>如何将一张图像视为一个模型的输入？</p>
<p>对于计算机系统而言，一张图像其实是一个<strong>三维</strong>的<strong>张量（tensor）</strong>。简单来讲，我们可以将维度大于2的矩阵视为张量。而这三维中，分别代表图片的<strong>宽、高和通道（Channel）的数目</strong>。那么，<strong>通道</strong>的意思是什么呢？一张彩色的图片，它的每一个像素点（Pixel）都是由RGB三种颜色组成，所以三个通道就代表了RGB三种颜色。而图片的宽和高就代表了这张图片的解析度，也就是像素点的数目。</p>
<p>我们将三维的张量“拉直”，就可以作为神经网络的输入了。对于这个“拉直”的向量，每一维的数值就代表了某一个位置的某一个颜色的强度。</p>
</li>
<li><p>在全连接网络（Fully Connected Network）中模型是如何工作的？</p>
<p>如图所示，我们将刚刚得到的向量（$100 \times 100\times 3$）作为输入，输入到全连接网络中。假设第一层的神经元（Neuron）的数目有1000个，</p>
</li>
<li></li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">机器学习之简要介绍</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-03 00:22:00" itemprop="dateCreated datePublished" datetime="2022-06-03T00:22:00+08:00">2022-06-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-04 17:49:26" itemprop="dateModified" datetime="2022-06-04T17:49:26+08:00">2022-06-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src="/2022/06/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/%E5%BC%95%E5%85%A5.png" class="" title="引入">
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h2 id="机器学习是什么？"><a href="#机器学习是什么？" class="headerlink" title="机器学习是什么？"></a>机器学习是什么？</h2><p>在网络上，我们经常听到关于机器学习、人工智能、深度学习的话题，那么，机器学习到底是什么？</p>
<p>简单来说，机器学习其实相当于构建一个映射函数。例如，在语音识别任务中，输入为语音片段，函数输出该语音片段对应的语言；在图像识别任务中，输入为一张图像，函数输出为该图像中的类别……</p>
<p>Tom Mitchell在1998年提出，”A computer problem is said to <strong>learn</strong> from <strong>experience $E$</strong> w.r.t. some <strong>task $T$</strong> and some <strong>performance measure $P$</strong>, if its performance on $T$, as measured by $P$, <strong>improves</strong> with experience $E$.”（对计算机系统而言，以<strong>性能量度 $P$ </strong>进行衡量，如果一个计算机程序在<strong>某类任务 $T$ </strong>上的性能，随着<strong>经验 $E$ </strong>而提升，那么我们称这个计算机程序从经验 $E$ 中学习。）</p>
<p>与之相比的是，对人类而言，学习是通过<strong>教授或体验</strong>而获得知识、技术、态度或价值的过程，从而导致<strong>可量度</strong>的<strong>稳定</strong>的行为变化。</p>
<img src="/2022/06/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/image-20220603004941595.png" class="" title="image-20220603004941595">
<h2 id="为什么要机器学习？"><a href="#为什么要机器学习？" class="headerlink" title="为什么要机器学习？"></a>为什么要机器学习？</h2><p>有一个很经典的例子，那就是对手写的数字进行图像识别：我们希望在给一个手写数字的图像给机器后，机器能够帮我们识别出，这个数字属于0~9之间的哪一个。</p>
<p>我们其实很难去编写一个程序来解决手写数字识别这样的问题。比如，我们如何区分2和7？简单的对每一个像素进行扫描识别他们的走势？这样的计算量极大，同时准确率也很低。而这个任务有一个很重要的特点：我们可以收集大量的包含了手写数字图像和对应数字的样本。</p>
<p>那么，我们是否可以通过学习这些样本，找到它们之间一些隐藏的特点，来生成一个能进行手写数字识别的程序？这就是机器学习的基本思路，而且，如果我们的工作足够好，这个程序也能识别新的手写数字的输入。</p>
<p>我们简单将为何要机器学习分为4点：</p>
<ol>
<li>缺乏相关的专门知识。对一些任务，我们拥有专门知识，那么就可以用数学工具进行分析，但是还有很多任务，我们缺乏相关的专门知识，因此，我们通过学习大量样本，找到隐含的特征（这些特征可能没有很好的可解释性），然后来预测之后新的样本。</li>
<li>难以解释的人类经验。比如人脸/手写/语音识别，我们很难去用数学语言解释大脑是如何进行这些任务的，还有开车、坐飞机这种经验也很难解释。</li>
<li>快速变化的现象。有一些现象会快速变化，在经济领域中有信用评分和财务建模等，在医学领域中有诊断学，在金融领域有欺诈检测等，在这也就意味着需要不断学习新的模型。</li>
<li>需要定制。对于不同的人，需要定制不同的需求，比如个性化新闻阅读器和电影/书籍推荐。</li>
</ol>
<h2 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h2><p><strong>机器学习按任务的分类：</strong></p>
<ol>
<li><strong>监督学习（Supervised Learning）</strong></li>
<li><strong>无监督学习（Unsupervised Learning）</strong></li>
<li><strong>强化学习（Reinforcement Learning）</strong></li>
</ol>
<h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>监督学习是指通过让机器学习大量<strong>带有标签的样本数据</strong>，训练出一个模型，并使该模型可以根据输入预测相应输出的过程。目标是根据<strong>标记数据（labeled data）</strong>学习<strong>特征（feature）与标签（label）</strong>之间的<strong>映射函数（mapping function）或关系（relationship）</strong>。</p>
<img src="/2022/06/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/image-20220604160144414.png" class="" title="image-20220604160144414">
<p>监督学习的预测目标为：</p>
<ul>
<li>分类（Classification）：输出为离散值</li>
<li>回归（Regression）：输出为连续值。</li>
</ul>
<p>例如对于一个普通的行人在街道行走的图像，如果是分类任务，我们的预测目标可以为行人和非行人，如果是回归任务，我们的预测目标可以是行人的目标。</p>
<p>常见的监督学习算法有：</p>
<ul>
<li>线性回归（Linear Regression）</li>
<li>罗辑斯特回归（Logistic Regression）</li>
<li>神经网络（Neural networks)</li>
<li>支持向量机（Support Vector Machines, SVMs）</li>
</ul>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习是直接对输入数据进行建模，提取数据有效信息探索数据的整体结构。</p>
<p>无监督学习用于：</p>
<ol>
<li>聚类（Clustering）</li>
<li>可视化（Visualization）和降维（Dimensionality reduction）</li>
<li>……</li>
</ol>
<p>常见的无监督学习算法有：</p>
<ol>
<li>K均值算法（K-means）（用于聚类）</li>
<li>主成分分析（Principle Component Analysis, PCA）（用于降维）</li>
</ol>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>强化学习是通过一个<strong>智能体</strong>在与复杂而不确定的<strong>环境</strong>交互中<strong>最大化总回报</strong>来学习的一种计算方法。智能体以“试错”的方式进行学习，通过与环境交互获得的<strong>奖赏和惩罚</strong>指导行为，目标是使智能体<strong>获得最大的奖赏</strong>。</p>
<p>典型的应用就是棋子对弈和机器人自动控制。</p>
<h2 id="机器学习基本流程"><a href="#机器学习基本流程" class="headerlink" title="机器学习基本流程"></a>机器学习基本流程</h2><p>机器学习是从有限的观测数据中学习（或者猜测）出具有一般性的规律，并且可以将总结出来的规律推广应用到未观测样本上。机器学习方法可以粗略的分为三个基本要素：模型、学习准则、优化算法。</p>
<p>机器模型的三要素：</p>
<ol>
<li>模型<ul>
<li>线性方法：$f(\mathbf{x},\theta)=\mathbf{w}^T \mathbf{x}+b$</li>
<li>广义线性方法：$f(\mathbf{x},\theta)=\mathbf{w}^T \phi(\mathbf{x})+b$ ，这里如果 $\phi(\mathbf{x})$ 为可学习的非线性基函数，$f(\mathbf{x},\theta)$ 就等价于神经网络。</li>
</ul>
</li>
<li>学习准则<ul>
<li>最小化期望风险：$\mathcal{R}(f)=\mathbf{E}_{(\mathbf{x},y) \sim p(\mathbf{x},y)} [\mathcal{L}(f(\mathbf{x},y)]$</li>
<li>最大似然估计</li>
<li>最大后验估计</li>
</ul>
</li>
<li>优化：梯度下降</li>
</ol>
<h3 id="学习准则"><a href="#学习准则" class="headerlink" title="学习准则"></a>学习准则</h3><p>令训练集 $\mathcal{D}={\lbrace \mathbf{x}^{(n)},y^{(n)} \rbrace}^N_{n=1}$ 是由 $N$ 个<strong>独立同分布</strong>的（Identically and Independently Distributed, IID）样本组成，即每个样本 $(\mathbf{x},y)$ 是从输入空间 $\mathcal{X}$ 和 输出空间 $\mathcal{Y}$  的联合空间中按照某个未知分布独立地随机产生的。这里要求样本分布必须是固定的（虽然可以是未知的），不会随着时间而变化。</p>
<p>一个好的模型 $f(\mathbf{x},\theta^*)$ 应该在所有 $(\mathbf{x},y)$ 的可能取值上都与真实映射函数 $y=g(\mathbf{x})$ 一致，即</p>
<script type="math/tex; mode=display">
|f(\mathbf{x},\theta^*)-y| < \epsilon ,\forall(\mathbf{x},y) \in \mathcal{X}\times\mathcal{Y}</script><p>或与真实条件概率分布 $p_r(y|\mathbf{x})$ 一致，即</p>
<script type="math/tex; mode=display">
|f_y(\mathbf{x},\theta^*)-p_r(y|\mathbf{x})| < \epsilon ,\forall(\mathbf{x},y) \in \mathcal{X}\times\mathcal{Y}</script><p>其中，$\epsilon$ 是一个很小的正数，$f_y(\mathbf{x},\theta^*)$ 为模型预测的条件概率分布中 $y$ 对应的概率。</p>
<p>模型 $f(\mathbf{x},\theta)$ 的好坏可以通过期望风险（Expected Risk）$\mathcal{R}(\theta)$​ 来衡量，其定义为</p>
<script type="math/tex; mode=display">
\mathcal{R}(\theta) = \mathbf{E}_{(\mathbf{x},y) \sim p_r(\mathbf{x},y)} [\mathcal{L}(y,f(\mathbf{x};\theta)]</script><p>其中，$p_r(\mathbf{x},y)$ 为真实的数据分布，$\mathcal{L}(y,f(\mathbf{x};\theta)$ 为损失函数，用于量化两个变量之间的差异。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><strong>损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异。</strong></p>
<p>下面介绍几种常用的损失函数：</p>
<ul>
<li>0-1 损失函数</li>
<li>平方损失函数</li>
<li>感知器损失函数</li>
<li>Hinge损失函数</li>
<li>交叉熵损失函数</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-22 19:30:46 / 修改时间：20:26:54" itemprop="dateCreated datePublished" datetime="2022-03-22T19:30:46+08:00">2022-03-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="高斯过程"><a href="#高斯过程" class="headerlink" title="高斯过程"></a>高斯过程</h1><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="深度学习的类型"><a href="#深度学习的类型" class="headerlink" title="深度学习的类型"></a>深度学习的类型</h3><p>（1）监督学习</p>
<p>（2）无监督学习</p>
<p>（3）增强学习</p>
<h3 id="与概率之间的关系"><a href="#与概率之间的关系" class="headerlink" title="与概率之间的关系"></a>与概率之间的关系</h3><p>有监督学习和无监督学习的区别就在于，求x的概率以及给定x求y的概率</p>
<h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><p>深度学习和贝叶斯模型的区别在于，深度学习需要大量的数据集，而贝叶斯模型，由于给定了先验概率，所以需要的数据没有那么大。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="数据不足"><a href="#数据不足" class="headerlink" title="数据不足"></a>数据不足</h3><p>Deterministic/Stochastic ：建立在物理模型上</p>
<p>Mechanistic/ Emipirical ：建立在经验模型上</p>
<p>高斯过程是随机过程的一种</p>
<p>物理模型不是足够精确的，数据不充分且模型不一定精确</p>
<h3 id="模型-p-t-的需求"><a href="#模型-p-t-的需求" class="headerlink" title="模型 $p(t)$ 的需求"></a>模型 $p(t)$ 的需求</h3><h3 id="高斯过程-1"><a href="#高斯过程-1" class="headerlink" title="高斯过程"></a>高斯过程</h3><h3 id="高斯分布的优良性质"><a href="#高斯分布的优良性质" class="headerlink" title="高斯分布的优良性质"></a>高斯分布的优良性质</h3><h2 id="高斯过程中的Kernel"><a href="#高斯过程中的Kernel" class="headerlink" title="高斯过程中的Kernel"></a>高斯过程中的Kernel</h2><p>构造核函数，希望能得到无限维的高斯分布来得到高斯过程</p>
<p>函数族，这些线性函数对应的是一个点</p>
<p>均值调整到(0,5)^T ，好像加了一些偏置，看起来有斜率了</p>
<p>20元的高斯分布，这里的每一条线是高斯分布的一个点</p>
<p>这时和机器学习有什么联系？</p>
<p>不够平滑，表现力不够强</p>
<p>平滑性</p>
<p>随机变量值 基本直觉 平方指数 核函数</p>
<p>用平方指数核函数来替代高斯分布里面的协方差矩阵 $cov(y_i;y_j)=\kappa (x_i;x_j)$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/" class="post-title-link" itemprop="url">机器学习之K均值、混合高斯模型和期望最大</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-21 10:03:28" itemprop="dateCreated datePublished" datetime="2022-03-21T10:03:28+08:00">2022-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-02 17:32:58" itemprop="dateModified" datetime="2022-06-02T17:32:58+08:00">2022-06-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习之K均值、混合高斯模型和期望最大"><a href="#机器学习之K均值、混合高斯模型和期望最大" class="headerlink" title="机器学习之K均值、混合高斯模型和期望最大"></a>机器学习之K均值、混合高斯模型和期望最大</h1><p>提纲</p>
<ul>
<li>聚类与K均值</li>
<li>混合高斯模型与期望最大</li>
<li>期望最大的另一个视角</li>
<li>期望最大的另一个视角</li>
<li>期望最大的理论基础</li>
</ul>
<h2 id="聚类与K均值"><a href="#聚类与K均值" class="headerlink" title="聚类与K均值"></a>聚类与K均值</h2><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>聚类是什么？举个例子，我们想把下图的人物归类，那么我们可以根据衣服样式归类，也可以根据性别、年龄等进行归类。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092330178.png" class="" title="image-20220321092330178">
<p>比如，我们想检测视频中的移动目标，这也是聚类的应用场景。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092356073.png" class="" title="image-20220321092356073">
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>由上面的示例，我们对聚类应该有了一个大致的意识。聚类的基本思想就是将相似的实例分组在一起。什么才叫做相似的实例？如果两个点之间的距离足够近，那我们就认为他们是相似的，就像下图这种2D点的模式。</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321092634112.png" class="" title="image-20220321092634112">
<p>两个点之间距离的衡量有很多种方法，在这篇文章中，我们取欧式距离，公式如下：</p>
<script type="math/tex; mode=display">
dist(\boldsymbol{x},\boldsymbol{x})=||\boldsymbol{x}-\boldsymbol{y}||_2^2</script><p>其中，$\boldsymbol{x}$ 是一个向量，$||\cdot||<em>2$ 是两个向量之间的二范数，其计算方法为 $||\boldsymbol{x}||_2=(\sum \limits</em>{i=1}^n x_i^2)^ \frac{1}{2}$ 。</p>
<p>我们的聚类结果取决于簇内相似度和簇间相似度，一般来说，我们希望簇内相似度更高，簇间相似度更低。</p>
<h3 id="聚类方法"><a href="#聚类方法" class="headerlink" title="聚类方法"></a>聚类方法</h3><ul>
<li>原型聚类——K均值算法、高斯混合聚类</li>
<li>密度聚类——DBSCAN算法、Mean-Shift算法</li>
<li>层次聚类——–Agglomerative 算法、Divisive算法、BIRCH 算法</li>
<li>谱聚类</li>
</ul>
<h3 id="聚类例子"><a href="#聚类例子" class="headerlink" title="聚类例子"></a>聚类例子</h3><p>我们举几个聚类所应用的场景。</p>
<ul>
<li><p>图像分割，目标：将图像分割成有意义的或感知上相似的区域</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094023139.png" class="" title="image-20220321094023139">
</li>
<li><p>基因表达数据聚类</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094031460.png" class="" title="image-20220321094031460">
<p><sup><a href="#fn_" id="reffn_"></a></sup>: Eisen et al,PNAS 1998</p>
</li>
<li><p>聚类新闻文章</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094102368.png" class="" title="image-20220321094102368">
</li>
<li><p>进化树</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321094107927.png" class="" title="image-20220321094107927">
<p><sup><a href="#fn_" id="reffn_"></a></sup>: Lindbald-Toh et al., Nature 2005</p>
</li>
</ul>
<h3 id="K均值聚类"><a href="#K均值聚类" class="headerlink" title="K均值聚类"></a>K均值聚类</h3><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>在 $D$ 维的欧式空间里，给定数据集 ${x<em>1,x_2,…,x_n}$ ，将其划分为 $K$ 个簇（类），这是给定的$K$ 个编码之一。同时引入指标变量 $r</em>{nk} \in \lbrace 0,1 \rbrace$，表示数据点 $x_n$ 属于K个聚类中的哪一个。</p>
<h4 id="失真度量"><a href="#失真度量" class="headerlink" title="失真度量"></a>失真度量</h4><p>K均值算法针对聚类所得簇 $C=\lbrace C_1,C_2,…,C_K \rbrace$ ，最小化平方误差和：</p>
<script type="math/tex; mode=display">
J=\sum \limits_{n=1}^N \sum \limits_{k=1}^K r_{nk} ||x_n-\mu_k||^2</script><p>其中，$\mu<em>k = \frac{1}{|C_k|} \sum \limits</em>{x \in C_k}x$ ，是簇 $C_k$ 的均值（$k = 1,2,…,K$）。</p>
<p>我们的目标其实就是找到 $\lbrace r_{nk} \rbrace$ 和 $\lbrace \mu_k \rbrace$ 的值，使得 $J$ 达到最小值。也就是说，我们希望找到对于每个簇 $C_k$ ，找到它的指标变量和均值，使得簇内每个点的距离最小。直观来看，这个式子在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，$J$ 值越小，簇内样本相似度越高。</p>
<p>我们可以⽤⼀种迭代的⽅法完成这件事，其中每次迭代涉及到两个连续的步骤，分别关于$\lbrace r_{nk} \rbrace$ 的最优化和 $\lbrace \mu_k \rbrace$ 的最优化。</p>
<h4 id="寻找-lbrace-r-nk-rbrace-的最优化和-lbrace-mu-k-rbrace-的最优化"><a href="#寻找-lbrace-r-nk-rbrace-的最优化和-lbrace-mu-k-rbrace-的最优化" class="headerlink" title="寻找$\lbrace r_{nk} \rbrace$ 的最优化和 $\lbrace \mu_k \rbrace$ 的最优化"></a>寻找$\lbrace r_{nk} \rbrace$ 的最优化和 $\lbrace \mu_k \rbrace$ 的最优化</h4><ol>
<li>首先选择 $\lbrace \mu_k \rbrace$ 的初始值；</li>
<li>第一阶段：我们关于 $r_{nk}$ 最小化 $J$，保持 $\mu_k$ 固定；</li>
<li>第二阶段：我们关于 $\mu<em>k$ 最小化 $J$，保持 $r</em>{nk}$ 固定；</li>
</ol>
<p>注：两个阶段分别对应于EM算法中的E（期望）步骤和M（最⼤化）步骤，EM算法稍后会介绍，这里只是引入这样的一个概念。</p>
<h4 id="E：参数-r-nk-的选择"><a href="#E：参数-r-nk-的选择" class="headerlink" title="E：参数 $r_{nk}$ 的选择"></a>E：参数 $r_{nk}$ 的选择</h4><ol>
<li><p>由于平方误差和 $J=\sum \limits<em>{n=1}^N \sum \limits</em>{k=1}^K r<em>{nk} ||x_n-\mu_k||^2$ 是一个关于 $r</em>{nk}$ 的线性函数（这里再次提醒，$r_{nk} \in \lbrace 0,1 \rbrace$，表示数据点 $x_n$ 属于 $K$ 个聚类中的哪一个），因此，这个优化问题存在解析解。</p>
</li>
<li><p>而显然，$J=\sum \limits<em>{n=1}^N \sum \limits</em>{k=1}^K r<em>{nk} ||x_n-\mu_k||^2 = \sum \limits</em>{k=1}^K r<em>{1k}||x_1-\mu_k||^2+…+ \sum \limits</em>{k=1}^K r<em>{Nk}||x_N-\mu_k||^2$ ，其中有不同的 $n$ 的项是独立的，因此，我们可以对每个 $n$ 分别进行最优化，只要 $k$ 的值使得 $||x_n-\mu_k||^2$ 最小，那么就使 $r</em>{nk}=1$。其实就是我们对于每一个数据点 $x<em>n$ ，都计算 $x_n$ 与每个簇 $C_k$ 的中心点（即 $\mu_k$）之间的距离，通过距离的比较，找到与 $x_n$ 最近的簇，将 $r</em>{nk}$ 置为1，表示这个数据点 $x_n$ 已经被分到簇 $C_k$ 中了。</p>
</li>
<li><p>因此，我们可以对参数 $r_{nk}$ 选择如下：</p>
<script type="math/tex; mode=display">
r_{nk} =
\begin{cases}
1,  & \text{if} \ k= \text{argmin}_j||x_n-\mu_j||^2 \\
0, & \text{otherwise}
\end{cases}</script></li>
<li><p>直观地说，其实就是将 $x_n$ 分到最近的一个类里面。</p>
</li>
</ol>
<h4 id="M：优化-mu-k"><a href="#M：优化-mu-k" class="headerlink" title="M：优化 $\mu_k$"></a>M：优化 $\mu_k$</h4><ol>
<li><p>固定 $r_{nk}$ ；</p>
</li>
<li><p>由于平方误差和 $J=\sum \limits<em>{n=1}^N \sum \limits</em>{k=1}^K r<em>{nk} ||x_n-\mu_k||^2$ 是一个关于 $\mu_k$ 的线性函数，（这里提醒一句，$\mu_k = \frac{1}{|C_k|} \sum \limits</em>{x \in C_k}x$ ，是簇 $C_k$ 的均值（$k = 1,2,…,K$）因此，我们只要令 $J$ 关于 $\mu_k$ 的导数等于零，$J$ 即可达到最小值。</p>
</li>
<li><p>求解过程为：</p>
<script type="math/tex; mode=display">
2\sum \limits_{n=1}^N r_{nk}(x_n-\mu_k)=0</script><p>解出结果为</p>
<script type="math/tex; mode=display">
\mu_k = \frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}</script></li>
<li><p>直观地说，其实就是令 $\mu<em>k$ 等于类别为 $k$ 的所有数据点的均值，$\sum_n r</em>{nk}$ 为类别为 $k$ 的所有数据点的数量，$\sum<em>n r</em>{nk}x_n$ 等于类别为 $k$ 的所有数据点的总和。</p>
</li>
</ol>
<h4 id="K均值算法终止"><a href="#K均值算法终止" class="headerlink" title="K均值算法终止"></a>K均值算法终止</h4><p>K均值算法，就是重复执行上述的选择 $r_{nk}$ 和优化 $\mu_k$ 的步骤，即重复分配数据点给簇（或类）和重复计算簇（或类）中心，直到聚类的分配不改变。由于每个阶段都减小了目标函数 $J$ 的值，因此算法的收敛性得到了保证。</p>
<h4 id="示例（来自PRML）"><a href="#示例（来自PRML）" class="headerlink" title="示例（来自PRML）"></a>示例（来自PRML）</h4><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210022034.png" class="" title="image-20220321210022034">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210028211.png" class="" title="image-20220321210028211">
<ul>
<li>$J$ 是老忠实间歇喷泉数据的代价函数。</li>
<li>我们故意将聚类中⼼选择了较差的初始值，从⽽算法在收敛之前执⾏了若⼲步。更好的选择是将聚类中⼼选择为由k个随机数据点组成的⼦集。</li>
<li>K均值算法本身经常被用于在EM算法之前初始化高斯混合模型的参数。（这里出现了一个新概念：混合高斯模型，什么是混合高斯模型，这就是接下来要介绍的。）</li>
</ul>
<h3 id="局部最小值"><a href="#局部最小值" class="headerlink" title="局部最小值"></a>局部最小值</h3><p>K均值算法的目标函数 $J=\sum \limits<em>{n=1}^N \sum \limits</em>{k=1}^K r_{nk} ||x_n-\mu_k||^2$ 是非凸的，因此，并不能保证 $J$ 上的坐标下降收敛到全局最小值，没有什么办法能够阻止K均值陷入局部极小值。我们可以尝试更多随机初始化的点，或者尝试非局部拆分和合并操作，非局部拆分就是同时合并相邻的两个簇，合并操作就是将一个大的簇分为两个。</p>
<h3 id="K均值算法的性质"><a href="#K均值算法的性质" class="headerlink" title="K均值算法的性质"></a>K均值算法的性质</h3><ol>
<li>保证在有限次迭代中收敛。</li>
<li>每次迭代的训练时间：将数据点分配到最近的簇中心是 $O(KN)$ 时间，根据分配的点改变聚类中心是 $O(N)$ 时间。</li>
</ol>
<h3 id="K均值的局限"><a href="#K均值的局限" class="headerlink" title="K均值的局限"></a>K均值的局限</h3><ul>
<li>在每⼀次迭代中，每个数据点被分配到⼀个唯⼀的聚类中（“硬”分配）  </li>
<li>某些点可能到两个聚类中心的距离相等</li>
<li>通过使⽤概率的⽅法，我们得到对数据点聚类的“软”分配，它反映出在最合适聚类分配上的不确定性  </li>
</ul>
<h4 id="图像分割与压缩"><a href="#图像分割与压缩" class="headerlink" title="图像分割与压缩"></a>图像分割与压缩</h4><p>之前提到，聚类算法其实可以用于图像分割与压缩。我们的目标是，将图像分割成若干的区域，每个区域有⼀个相对相似的视觉外观，或者对应于某个物体，或物体的⼀部分。在计算机图像中，每个像素是⼀个3维空间中的⼀个点，三维空间由{红、绿、蓝} 通道的三个亮度值构成，每个值由8比特的精度存储。</p>
<p>K均值聚类与K个颜色的调色板一起使用，该方法没有考虑不同像素的空间上的近似性。</p>
<h5 id="（1）图像分割"><a href="#（1）图像分割" class="headerlink" title="（1）图像分割"></a>（1）图像分割</h5><p>以下是选择2、3和10种颜色对彩色图像进行编码的两个示例：</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220321210941474.png" class="" title="image-20220321210941474">
<p>（2）数据压缩</p>
<ul>
<li><p>无损数据压缩：能够从压缩的表⽰中精确地重建原始数据。</p>
</li>
<li><p>有损数据压缩：接受重建过程中出现的⼀些错误。</p>
</li>
<li><p>K-Means应用于有损数据压缩</p>
<ul>
<li>对于 $ N $ 个数据点中的每一个，我们只存储它被分配的聚类种类 $k$。</li>
<li>我们还存储了 $K$个聚类中心 $μ_k$ 的值，这通常需要存储小得多的数据,其中我们假定 $K ≪ N$。这样，每个数据点都根据它最近的中心 $μ_k$ 确定。新的数据点可以类似地压缩。</li>
<li>首先找到最近的 $μ_k$，然后存储标签 $k$ 而不是原始的数据向量</li>
</ul>
<p>这个框架被称为向量量子化(vector quantization)，向量 $μ_k$ 被称为编码书向量 (code-book vector)。</p>
</li>
<li><p>上面讨论的图像分割问题也说明了数据压缩中聚类的使用。</p>
<ul>
<li>假设原始图像有 $ N $ 个像素，每个像素由$ {R, G, B} $三个值组成，每个值由$ 8 $比特的精度存储。这样，直接传递整幅图像需要$ 24N $比特。</li>
<li>现在假设我们首先在图像数据上运行K均值算法，然后，我们不直接传递原始像素亮度向量，而是传递最近的向量$ \mu_k $的亮度。由于有$ K $个这样的向量，因此每个像素需要$ \log_2K $比特。</li>
<li>我们还必须传送$ K $个编码书向量$ \mu_k $，这需要$ 24K $比特</li>
<li>因此传递这个图像所需的比特总数为$ 24K + N \log_2K $（四舍五入到最近的整数）</li>
<li>当 $K=2,3$ 以及 $10$ 时，压缩为原来的比例分别为4%，8%，17%</li>
</ul>
</li>
</ul>
<h2 id="混合高斯模型与期望最大"><a href="#混合高斯模型与期望最大" class="headerlink" title="混合高斯模型与期望最大"></a>混合高斯模型与期望最大</h2><h3 id="单高斯模型"><a href="#单高斯模型" class="headerlink" title="单高斯模型"></a>单高斯模型</h3><p>一维高斯（正态）分布的概率密度函数如下：</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \text{exp}(-\frac{(x-\mu)^2}{2\sigma^2})</script><p>其中，$\mu$ 和 $\sigma^2$ 分别是高斯分布的均值和方差。</p>
<h3 id="二维高斯模型"><a href="#二维高斯模型" class="headerlink" title="二维高斯模型"></a>二维高斯模型</h3><p>多维变量 $X=\lbrace x_1,x_2,…,x_n \rbrace$ 的联合概率密度函数为：</p>
<script type="math/tex; mode=display">
f(X)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(X-\mu)^T \Sigma ^{-1}(X-\mu)],X=(x_1,x_2,...,x_n)</script><p>其中，$d$ 为变量维度，对于二维高斯分布，有 $d=2$ ；$u = \begin{pmatrix} u_1 \ u_2 \ …\ u_n\end{pmatrix}$ ，为各维变量的均值，$\Sigma$ 为协方差矩阵，描述各维变量之间的相关度，对于二维高斯分布，有</p>
<script type="math/tex; mode=display">
\Sigma = \begin{bmatrix} \delta_{11} \ \delta_{12} \\ \delta_{21} \ \delta_{22}\end{bmatrix}</script><h3 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型 (GMM)"></a>高斯混合模型 (GMM)</h3><p>一个高斯混合模型能表示成如下分布：</p>
<script type="math/tex; mode=display">
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)</script><p>其中 $\mathcal{N}(x|\mu<em>k,\Sigma_k)$ 就是上面的多维变量的高斯模型。而混合系数 $\pi_k$ 满足 $0 \le \pi_k \le 1$ 和 $\sum \limits</em>{k=1}^K \pi_k = 1$ 。</p>
<p>某个混合高斯模型融合的高斯模型个数足够多，它们之间的权重设定得足够合理，这个混合模型可以拟合任意分布的样本。</p>
<h3 id="一维高斯混合函数的边缘分布"><a href="#一维高斯混合函数的边缘分布" class="headerlink" title="一维高斯混合函数的边缘分布"></a>一维高斯混合函数的边缘分布</h3><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224144606.png" class="" title="image-20220328224144606">
<p>⾼斯混合概率分布可以写成高斯分布的线性叠加的形式</p>
<h3 id="二维高斯混合函数的边缘分布"><a href="#二维高斯混合函数的边缘分布" class="headerlink" title="二维高斯混合函数的边缘分布"></a>二维高斯混合函数的边缘分布</h3><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E5%9D%87%E5%80%BC%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7/image-20220328224207091.png" class="" title="image-20220328224207091">
<h3 id="GMMs-和隐变量"><a href="#GMMs-和隐变量" class="headerlink" title="GMMs 和隐变量"></a>GMMs 和隐变量</h3><p>我们引入离散潜在变量 $z$ 来描述高斯混合模型，更加深刻地认识这个重要的分布，开始了解期望最大化算法。</p>
<p>首先，我们已经知道，$K$ 个高斯分布的线性叠加</p>
<script type="math/tex; mode=display">
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)</script><p>然后，我们可以引入一个 $K$ 维的二维随机变量 $z$ ，用 1-of-K 表示（one-hot 变量），即 $z = \lbrace z_1,z_2,…,z_k \rbrace$ ，其中 $z_k \in \lbrace 0,1 \rbrace$ 且 $\sum_k z_k=1$  ，向量 $z$ 有 $K$ 种可能的状态。</p>
<p>那么，我们就可以确定边缘概率分布 $p(z)$ 了。</p>
<h3 id="确定边缘概率分布-p-z"><a href="#确定边缘概率分布-p-z" class="headerlink" title="确定边缘概率分布 $p(z)$"></a>确定边缘概率分布 $p(z)$</h3><p>我们将概率与每个组件 $z_k$ 相关联。那么它们的关系要怎么连接呢？将 $z$ 的边缘概率分布根据混合系数$π_k$ 进⾏赋值，即</p>
<script type="math/tex; mode=display">
p(z_k = 1)=\pi_k</script><p>其中 $k$ 满足 $0 \le \pi_k \le 1$ 且 $\sum_k \pi_k = 1$ 。</p>
<p>由于 $z$ 使用了“ 1-of-K ”  的表示方法，因此 $z$ 的边缘概率分布也可以写成</p>
<script type="math/tex; mode=display">
p(z)=\prod \limits_{k=1}^K \pi_k^{z_k}</script><p>当有1个分量时，$p(z_1) = \pi_k^{z_1}$ ；当有2个分量时，$p(z_1,z_2)=\pi_1^{z_1}\pi_2^{z_2}$ 。</p>
<h3 id="确定条件概率-p-x-z"><a href="#确定条件概率-p-x-z" class="headerlink" title="确定条件概率 $p(x|z) $"></a>确定条件概率 $p(x|z) $</h3><p>给定 $z$ 的⼀个特定的值，$x$ 的条件概率分布是⼀个高斯分布</p>
<script type="math/tex; mode=display">
p(x|z_k=1) = \mathcal{N}(x|\mu_k,\Sigma_k)</script><p>因此，条件概率  $p(x|z) $可以写成如下形式：</p>
<script type="math/tex; mode=display">
p(x|z) = \prod \limits_{k=1}^K \mathcal{N}(x|\mu_k,\Sigma_k)^{z_k}</script><p>由于指数 $z_k$ 的存在，这个连乘中除了一个乘积项之外其余的都等于1.</p>
<h3 id="联合分布"><a href="#联合分布" class="headerlink" title="联合分布"></a>联合分布</h3><p>定义隐变量 $z$ 和观测变量 $x$ 的联合分布</p>
<script type="math/tex; mode=display">
p(x, z)=p(z)p(x|z)</script><p>其中，$x$ 为观测变量，$z$ 为隐变量，$p(z)$ 为边缘分布，$p(x|z)$为条件分布。</p>
<h3 id="边缘概率分布p-x"><a href="#边缘概率分布p-x" class="headerlink" title="边缘概率分布p(x)"></a>边缘概率分布p(x)</h3><p>由于联合分布是 $p(x, z)=p(z)p(x|z)$ ，因此，$x$ 的边缘分布可以通过将联合概率分布对所有可能的 $z$ 求和的方式得到，即 </p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x) & = \sum\limits_zp(z)p(x|z) \\ 
& = \sum\limits_z(\prod \limits_{k=1}^K \pi_k^{z_k} \mathcal{N}(x|\mu_k,\Sigma_k)^{z_k}) \\
 & = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\end{aligned}</script><p>$x$ 的分布是高斯混合模型的标准形式。</p>
<p>对联合概率分布 $p(x, z)$ 操作，⽽不是对边缘概率分布 $p(x)$ 操作，这会产⽣极⼤的计算上的简化。</p>
<h3 id="另一个条件概率-Responsibility-p-z-x"><a href="#另一个条件概率-Responsibility-p-z-x" class="headerlink" title="另一个条件概率 (Responsibility)$p(z|x)$"></a>另一个条件概率 (Responsibility)$p(z|x)$</h3><p>另一个起着重要作用的量是给定$ x $的条件下，$ z $的条件概率。我们会用$ \gamma(z<em>k) $表示$ p(z_k=1|x) $，它的值可以使用贝叶斯定理求出<br>$ \begin{eqnarray} \gamma(z_k) \equiv p(z_k = 1|x) &amp; = &amp; \frac{p(z_k = 1)p(x|z_k = 1)}{\sum\limits</em>{j=1}^Kp(z<em>j=1)p(x|z_j = 1)} \ &amp; = &amp; \frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum\limits</em>{j=1}^K\pi_j\mathcal{N}(x|\mu_j,\Sigma_k)}\end{eqnarray} $<br>我们将 $ \pi_k $ 看成 $ z_k = 1 $ 的先验概率，将 $ \gamma(z_k) $ 看成观测到$ x $之后，对应的后验概率。正如我们将看到的那样，$ \gamma(z_k) $ 也可以被看做分量 $ k $ 对于“解释”观测值  $x$  的“责任”（responsibility）。</p>
<h3 id="GMM的最大似然"><a href="#GMM的最大似然" class="headerlink" title="GMM的最大似然"></a>GMM的最大似然</h3><p>假设我们有⼀个观测的数据集 $x<em>1,x_2,…,x_N$ ，我们希望使用混合高斯模型来对数据进行建模（ $N$ 条数据，维度都为 $D$ ），则数据集可以表示成 $N×D$ 的矩阵 $X = \begin{bmatrix} \mathbf{x}_1 \ \mathbf{x}_2 \ … \\mathbf{x}_N\end{bmatrix}$  ， $\mathbf{x}_n = [\mathbf{x}</em>{n1} \ \mathbf{x}<em>{n2} \ …\  \mathbf{x}</em>{nD}]$ 。对应隐含变量会被表示为⼀个 $N×K$ 的矩阵 $Z = \begin{bmatrix} \mathbf{z}<em>1 \ \mathbf{z}_2 \ … \\mathbf{z}_N\end{bmatrix}$， $\mathbf{z}_n = [\mathbf{z}</em>{n1} \ \mathbf{z}<em>{n2} \ …\  \mathbf{z}</em>{nK}]$。</p>
<p>目标是表示这个似然函数，从而通过最大化似然函数来估计这三组参数 $\pi_k,\mu_k,\Sigma_k$</p>
<p>混合密度函数为</p>
<script type="math/tex; mode=display">
p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)</script><p>因此，对数似然函数为</p>
<script type="math/tex; mode=display">
\ln p(\boldsymbol{X} \mid \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}</script><p>这是一个比单一高斯更难的问题。</p>
<h3 id="最大化对数似然函数"><a href="#最大化对数似然函数" class="headerlink" title="最大化对数似然函数"></a>最大化对数似然函数</h3><p>目标是估计以下三个参数集 $\pi_k,\mu_k,\Sigma_k$​ ，Ø在保持其他导数不变的情况下，依次求导数，但是没有解析解。这项任务并不简单，因为对k的求和出现在对数计算内部，从而对数函数不再直接作用于高斯分布。虽然基于梯度的优化是可能的，我们考虑更一般的迭代EM算法。</p>
<h3 id="GMM-最大似然估计的一些问题"><a href="#GMM-最大似然估计的一些问题" class="headerlink" title="GMM 最大似然估计的一些问题"></a>GMM 最大似然估计的一些问题</h3><p>在最大化似然函数之前，简单地提两个技术问题：</p>
<p>• 病态解，即高斯混合的奇异性问题。</p>
<p>• 混合的可区分问题</p>
<h4 id="病态解"><a href="#病态解" class="headerlink" title="病态解"></a>病态解</h4><p>我们考虑一个高斯混合模型，它的分量的协方差矩阵为 $ \Sigma_k = \sigma_k^2I $，其中$ I $是一个单位矩阵，结论对于一般的协方差矩阵仍然成立。假设混合模型的第 $ j $ 个分量的均值 $ \mu_j $ 与某个数据点完全相同，即对于某个 $ n $ 值，$ \mu_j = x_n $ 。这样，这个数据点会为似然函数贡献一项，形式为</p>
<script type="math/tex; mode=display">
\mathcal{N}(x_n|x_n,\sigma_j^2I) = \frac{1}{(2\pi)^{D/2}}\frac{1}{\sigma_j^D}</script><p>如果我们考虑极限$ \sigma_j \to 0 $，那么我们看到这一项趋于无穷大，因此对数似然函数也会趋于无穷大。</p>
<p>回忆一下，这个问题在单一的高斯分布中没有出现。为了理解不同之处，我们注意到，如果单一的高斯分布退化到了一个数据点上，那么它总会给由其他数据点产生的似然函数贡献可乘的因子，这些因子会以指数的速度趋于0，从而使得整体的似然函数趋于零而不是无穷大。</p>
<p>使⽤合适的启发式⽅法可以避免这个问题：重置均值或协方差。</p>
<h4 id="可区分问题"><a href="#可区分问题" class="headerlink" title="可区分问题"></a>可区分问题</h4><p>对于任意给定的最大似然解，一个由 $K$ 个分量混合而成的概率分布总共会有 $K!$ 个等价的解，对应于 $K!$ 种将 $K$ 个参数集合分配到 $K$ 个分量上的方式。换句话说，对于参数值空间中任意给定的点，都会有 $K!−1$ 个其他的点给出完全相同的概率分布。这个问题被称为可区分（identifiability）问题。但是，这个问题与找到一个好的概率模型无关，因为任意等价的解互相之间都一样好。</p>
<h3 id="用于高斯混合模型的EM"><a href="#用于高斯混合模型的EM" class="headerlink" title="用于高斯混合模型的EM"></a>用于高斯混合模型的EM</h3><p>终于突入这个问题了！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">机器学习之线性回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-21 00:07:10" itemprop="dateCreated datePublished" datetime="2022-03-21T00:07:10+08:00">2022-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-23 21:08:55" itemprop="dateModified" datetime="2022-06-23T21:08:55+08:00">2022-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习之线性回归"><a href="#机器学习之线性回归" class="headerlink" title="机器学习之线性回归"></a>机器学习之线性回归</h1><p>在准备华为昇腾RCNN的课程设计，暂时没有时间打公式，凑活着看吧=。=</p>
<p>之后会解释每个式子的直观解释的！</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>回归问题的最简单模型是输入变量的线性组合。</p>
<script type="math/tex; mode=display">
y(\boldsymbol{x},\boldsymbol{w})=w_0+w_1x_1+...+w_Dx_D</script><p>其中，$\boldsymbol{x}=(x_1,…,x_D)^T$</p>
<p>称为线性回归（linear regression）。</p>
<p>这个模型的关键性质：是参数 $w_0, …,w_D$ 的⼀个线性函数。</p>
<h3 id="一般形式"><a href="#一般形式" class="headerlink" title="一般形式"></a>一般形式</h3><ul>
<li>线性模型一般形式</li>
<li>向量形式</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><script type="math/tex; mode=display">
f(\boldsymbol{x};\boldsymbol{w},b)=\boldsymbol{w}^T\boldsymbol{x}+b</script><ul>
<li>增广权重向量和增广特征向量 $f(\boldsymbol{x};\hat{\boldsymbol{w}})=\hat{\boldsymbol{w}}^T\hat{\boldsymbol{x}}$</li>
</ul>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234627441.png" class="" title="image-20220316234627441">
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234640197.png" class="" title="image-20220316234640197">
<h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><ul>
<li><p>一元线性回归</p>
<script type="math/tex; mode=display">
f(x)=wx_i+b \text{使得}f(x_i)\simeq y_i</script></li>
<li><p>参数/模型估计：最小二乘法（least square method）</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234834443.png" class="" title="image-20220316234834443">
</li>
<li><p>最小化均方误差</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234848017.png" class="" title="image-20220316234848017">
</li>
<li><p>分别对w和b求导</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234858233.png" class="" title="image-20220316234858233">
</li>
<li><p>得到闭式解</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234908998.png" class="" title="image-20220316234908998">
<p>其中，</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234919459.png" class="" title="image-20220316234919459">
</li>
</ul>
<h3 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h3><ul>
<li><p>给定数据集</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234952472.png" class="" title="image-20220316234952472">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316234959106.png" class="" title="image-20220316234959106">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316235004841.png" class="" title="image-20220316235004841">
</li>
</ul>
<ul>
<li><p>多元线性回归目标</p>
<script type="math/tex; mode=display">
f(\boldsymbol{x}_i)=\boldsymbol{w}^T\boldsymbol{x}_i+b \text{使得}f(\boldsymbol{x}_i)\simeq y_i</script></li>
<li><p>把 w 和 b 吸收进入向量形式，数据集表示为</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316235125794.png" class="" title="image-20220316235125794">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316235133775.png" class="" title="image-20220316235133775">
</li>
<li><p>最小二乘法（least square method）</p>
<ul>
<li><script type="math/tex; mode=display">
\hat{\boldsymbol{w}}^{*}= \text{argmin}_{\hat{w}} \left( \boldsymbol{y}- \mathbf{X} \hat{ \boldsymbol{w}}^{ \boldsymbol{T}} \right) \left( \boldsymbol{y} - \mathbf{X} \hat{ \boldsymbol{w}} \right)</script></li>
<li><p>令 $E_{\hat{\boldsymbol{w}}}=\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)^{\boldsymbol{T}} \left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)$ ，对 $\hat{\boldsymbol{w}}$ 求导得到</p>
<script type="math/tex; mode=display">
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} = 2\mathbf{X}^{\rm{T}}\left(\mathbf{X}\hat{\boldsymbol{w}}-\boldsymbol{y}\right)</script></li>
<li><p>令上式为零可得 $\hat{\boldsymbol{w}}$ 最优解的闭式解</p>
<script type="math/tex; mode=display">
\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\rm{T}}\mathbf{X}\right)^{-1} \mathbf{X}^{\boldsymbol{T}}\boldsymbol{y}</script><p>又称伪逆。</p>
</li>
</ul>
</li>
</ul>
<h3 id="满秩讨论"><a href="#满秩讨论" class="headerlink" title="满秩讨论"></a>满秩讨论</h3><ul>
<li><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220316235812685.png" class="" title="image-20220316235812685"></li>
<li><p>$\mathbf{X}^{\rm{T}}\mathbf{X} $ 不是满秩矩阵：</p>
<ul>
<li>奇异值分解（SVD）</li>
<li>引入正则化</li>
</ul>
</li>
<li><p>并不是所有的函数都能通过输入变量直接线性地近似</p>
</li>
<li><img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000002862.png" class="" title="image-20220317000002862"></li>
<li>这样的情况下，我们需要采用输入变量的多项式或其它项来拟合函数，这样还是线性回归吗？</li>
<li>是的，只要对于系数是线性的就仍然是线性回归问题。</li>
</ul>
<h3 id="基函数"><a href="#基函数" class="headerlink" title="基函数"></a>基函数</h3><ul>
<li>在一些应用中，我们对原始数据变量进行一些预处理或特征提取。</li>
<li>比如原始变量为x，特征可以表示为基函数的形式{φ(x)}。</li>
<li>通过采用基函数，我们使得线性模型可以是输入变量的非线性函数。</li>
</ul>
<h3 id="扩展模型"><a href="#扩展模型" class="headerlink" title="扩展模型"></a>扩展模型</h3><ul>
<li><p>将输⼊变量的固定的⾮线性函数进⾏线性组合</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000139983.png" class="" title="image-20220317000139983">
<p>$\phi_j(\boldsymbol{x})$ 被称为基函数（basis function）。</p>
</li>
<li><p>模型中的参数总数为 $M$</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000231536.png" class="" title="image-20220317000231536">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000236259.png" class="" title="image-20220317000236259">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000243397.png" class="" title="image-20220317000243397">
</li>
</ul>
<h3 id="基函数的选择"><a href="#基函数的选择" class="headerlink" title="基函数的选择"></a>基函数的选择</h3><p>多项式曲线拟合</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000339208.png" class="" title="image-20220317000339208">
<p>其中，$M$ 是多项式的阶数（order），$x^j$ 表⽰ $x$ 的 $j$ 次幂，多项式系数 $w_0 ,…,w_M$ 整体记作向量 $\boldsymbol{w}$，多项式函数 $y(x,\boldsymbol{w})$ 是 $x$ 的⼀个⾮线性函数，是系数 $\boldsymbol{w}$ 的⼀个线性函数。</p>
<ul>
<li><p>多项式基函数</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000607775.png" class="" title="image-20220317000607775">
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000617476.png" class="" title="image-20220317000617476">
</li>
<li><p>“高斯”基函数</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000625242.png" class="" title="image-20220317000625242">
<ul>
<li>参数μj控制了基函数在输⼊空间中的位置，</li>
<li>参数s控制了基函数的空间⼤⼩</li>
</ul>
</li>
<li><p>sigmoid基函数</p>
<img src="/2022/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20220317000650443.png" class="" title="image-20220317000650443">
</li>
</ul>
<h2 id="最大似然与最小平方"><a href="#最大似然与最小平方" class="headerlink" title="最大似然与最小平方"></a>最大似然与最小平方</h2><h3 id="最大似然与最小平方-1"><a href="#最大似然与最小平方-1" class="headerlink" title="最大似然与最小平方"></a>最大似然与最小平方</h3><h3 id="最大似然"><a href="#最大似然" class="headerlink" title="最大似然"></a>最大似然</h3><h3 id="boldsymbol-w-的解"><a href="#boldsymbol-w-的解" class="headerlink" title="$\boldsymbol{w}$的解"></a>$\boldsymbol{w}$的解</h3><h3 id="偏置参数-w-0"><a href="#偏置参数-w-0" class="headerlink" title="偏置参数$w_0$"></a>偏置参数$w_0$</h3><h3 id="噪声精度-beta"><a href="#噪声精度-beta" class="headerlink" title="噪声精度$\beta$"></a>噪声精度$\beta$</h3><h2 id="正则化最小平方"><a href="#正则化最小平方" class="headerlink" title="正则化最小平方"></a>正则化最小平方</h2><h3 id="正则化最小平方-1"><a href="#正则化最小平方-1" class="headerlink" title="正则化最小平方"></a>正则化最小平方</h3><h3 id="最简单正则化项——权值衰减"><a href="#最简单正则化项——权值衰减" class="headerlink" title="最简单正则化项——权值衰减"></a>最简单正则化项——权值衰减</h3><h3 id="岭回归的闭式解"><a href="#岭回归的闭式解" class="headerlink" title="岭回归的闭式解"></a>岭回归的闭式解</h3><h3 id="正则化项的几何解释"><a href="#正则化项的几何解释" class="headerlink" title="正则化项的几何解释"></a>正则化项的几何解释</h3><h3 id="岭回归的最优解"><a href="#岭回归的最优解" class="headerlink" title="岭回归的最优解"></a>岭回归的最优解</h3><h3 id="更加⼀般的正则化项"><a href="#更加⼀般的正则化项" class="headerlink" title="更加⼀般的正则化项"></a>更加⼀般的正则化项</h3><h3 id="正则化方法结论"><a href="#正则化方法结论" class="headerlink" title="正则化方法结论"></a>正则化方法结论</h3><p>```</p>
<h2 id="偏置-方差分解"><a href="#偏置-方差分解" class="headerlink" title="偏置-方差分解"></a>偏置-方差分解</h2><h3 id="回顾回归的决策论"><a href="#回顾回归的决策论" class="headerlink" title="回顾回归的决策论"></a>回顾回归的决策论</h3><h3 id="偏置-方差分解-1"><a href="#偏置-方差分解-1" class="headerlink" title="偏置-方差分解"></a>偏置-方差分解</h3><h3 id="偏置-方差折中"><a href="#偏置-方差折中" class="headerlink" title="偏置-方差折中"></a>偏置-方差折中</h3><h3 id="偏置"><a href="#偏置" class="headerlink" title="偏置"></a>偏置</h3><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><h3 id="监督学习中方差的来源"><a href="#监督学习中方差的来源" class="headerlink" title="监督学习中方差的来源"></a>监督学习中方差的来源</h3><h3 id="偏置-方差之间的权衡"><a href="#偏置-方差之间的权衡" class="headerlink" title="偏置-方差之间的权衡"></a>偏置-方差之间的权衡</h3><h2 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/19/LinearProgramming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/19/LinearProgramming/" class="post-title-link" itemprop="url">LinearProgramming</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-19 17:30:41" itemprop="dateCreated datePublished" datetime="2022-03-19T17:30:41+08:00">2022-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-23 19:31:03" itemprop="dateModified" datetime="2022-06-23T19:31:03+08:00">2022-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="组合设计与组合优化"><a href="#组合设计与组合优化" class="headerlink" title="组合设计与组合优化"></a>组合设计与组合优化</h1><h2 id="线性规划"><a href="#线性规划" class="headerlink" title="线性规划"></a>线性规划</h2><h3 id="1-线性规划问题的数学模型"><a href="#1-线性规划问题的数学模型" class="headerlink" title="1 线性规划问题的数学模型"></a>1 线性规划问题的数学模型</h3><p>线性规划是最简单，应用最广泛的一种数学规划方法，也是使用最早的一种优化方法。从数学上说，线性规划问题可以归结为一类条件极值问题，用微积分方法来解决一般是无能为力的。</p>
<p>线性规划问题可归结为一类条件极值问题，即：在一组线性约束条件下，寻求一个线性函数的极大值或极小值。</p>
<p>目标函数：</p>
<script type="math/tex; mode=display">
\text{max(min)} \ y = \sum \limits_{i=1}^r c_ix_i</script><p>约束条件：</p>
<script type="math/tex; mode=display">
\begin{align}
s.t. \sum \limits_{i=1}^r a_{1i}x_i & \le(=,\ge) b_1 \\\\
 \sum \limits_{i=1}^r a_{2i}x_i & \le(=,\ge) b_2 \\\\
 ......\\\\
  \sum \limits_{i=1}^r a_{2mi}x_i & \le(=,\ge) b_m \\\\
\end{align}</script><p>非负约束条件：</p>
<script type="math/tex; mode=display">
x_i \ge 0,i = 1,2,...,r</script><h4 id="（1）运输问题"><a href="#（1）运输问题" class="headerlink" title="（1）运输问题"></a>（1）运输问题</h4><p>设有两个电视机厂 A1,A2，产量分别为 23 万台与 27 万台，其产品供应三个城市 B1,B2,B3。每个城市的需要量分别为 17 万台，18 万台和 15 万台，而各厂到各城市的运费单价如下表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>B₁</th>
<th>B₂</th>
<th>B₃</th>
</tr>
</thead>
<tbody>
<tr>
<td>A₁</td>
<td>50</td>
<td>60</td>
<td>70</td>
</tr>
<tr>
<td>A₂</td>
<td>60</td>
<td>110</td>
<td>160</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>B₁</th>
<th>B₂</th>
<th>B₃</th>
<th>总发量</th>
</tr>
</thead>
<tbody>
<tr>
<td>A₁</td>
<td>X₁₁</td>
<td>X₁₂</td>
<td>X₁₃</td>
<td>23</td>
</tr>
<tr>
<td>A₂</td>
<td>X₂₁</td>
<td>X₂₂</td>
<td>X₂₃</td>
<td>27</td>
</tr>
<tr>
<td>总收量</td>
<td>17</td>
<td>18</td>
<td>15</td>
<td>50</td>
</tr>
</tbody>
</table>
</div>
<p>则我们可以定义其为一个线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{min} \ y = 50x_{11}+60x_{12}+70x_{13}+60x_{21}+110x_{22}+160x_{23} \\\\
\begin{aligned}
s.t. \quad x_{11} + x_{12} + x_{13} & = 23 \\\\
x_{21} + x_{22} + x_{23} & = 27 \\\\
x_{11} + x_{21} & = 17 \\\\
x_{12} + x_{22} & = 18 \\\\
x_{13} + x_{23} & = 15 \\\\
\end{aligned}
\\\\ x_{ij}  \ge 0,i=1,2;j=1,2,3
\end{split}
\end{equation}</script><p>（2）生产计划问题</p>
<p>我们需要加工 4 种产品，分别为 P1,P2,P3,P4，而我们拥有三种机床 A,B,C ，如果要加工产品就必须同时使用 3 种机床进行加工。而机床类型，机床数量以及每种机床加工产品的时间如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>机床类型</th>
<th>机床数量</th>
<th>P1（小时）</th>
<th>P2（小时）</th>
<th>P3（小时）</th>
<th>P4（小时）</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>20</td>
<td>2</td>
<td>2</td>
<td>0.5</td>
<td>1.5</td>
</tr>
<tr>
<td>B</td>
<td>30</td>
<td>0.5</td>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>C</td>
<td>15</td>
<td>1.5</td>
<td>1</td>
<td>3</td>
<td>1.5</td>
</tr>
</tbody>
</table>
</div>
<p>每种产品的利润如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>商品</th>
<th>P1</th>
<th>P2</th>
<th>P3</th>
<th>P4</th>
</tr>
</thead>
<tbody>
<tr>
<td>单个利润</td>
<td>3.5 元</td>
<td>4.2 元</td>
<td>6.5 元</td>
<td>3.8 元</td>
</tr>
</tbody>
</table>
</div>
<p>每台机床每周运行不超过 60 小时，为了使获得的利润达到最大，问每周应制造这些产品各多少件？</p>
<p>设 $x_i$ 为 每周制造产品 $P_i$ 的数量，我们可以定义其为一个线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = 3.5 x_1 + 4.2 x_2 + 6.5 x_3 + 3.8 x_4 \\\\
\begin{aligned}
s.t. 2x_{1} + 2x_{2} + 0.5x_{3} + 1.5x_{4} & \le 1200 \\\\
0.5x_{1} + 2x_{2} + x_{3} + 2x_{4} & \le 1800 \\\\
1.5x_{1} + x_{2} + 3x_{3} + 1.5x_{4} & \le 800
\end{aligned}\\\\
x_{i}  \ge 0,i=1,2,3,4
\end{split}
\end{equation}</script><h4 id="（3）网络流问题"><a href="#（3）网络流问题" class="headerlink" title="（3）网络流问题"></a>（3）网络流问题</h4><p>在之前所讨论的寻求运输网络中最大流的问题，实际上是一个线性规划问题。</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ f_v = \sum \limits_{j \in V} f(s,j) \\\\
\begin{aligned}
s.t.\ &  c(i,j) - f(i,j)  \ge 0 \\\\
& \sum \limits_{\substack{i \in V \\\\ j \neq s,t}} f(i,j) - \sum \limits_{ \substack{j \in V \\\\ j \neq s,t}} f(j,k)  = 0 \\\\
 & f(i,j) \ge 0,(i,j) \in E
\end{aligned}
\end{split}
\end{equation}</script><h3 id="2-线性规划问题的几何意义"><a href="#2-线性规划问题的几何意义" class="headerlink" title="2 线性规划问题的几何意义"></a>2 线性规划问题的几何意义</h3><h4 id="（1）相关概念"><a href="#（1）相关概念" class="headerlink" title="（1）相关概念"></a>（1）相关概念</h4><p><strong>可行解</strong>：满足以下约束条件</p>
<script type="math/tex; mode=display">
\begin{split}
\begin{aligned}
s.t. \sum \limits_{i=1}^r a_{1i}x_i & \le(=,\ge) b_1 \\\\
 \sum \limits_{i=1}^r a_{2i}x_i & \le(=,\ge) b_2 \\\\
 ......\\\\
  \sum \limits_{i=1}^r a_{2mi}x_i & \le(=,\ge) b_m
\end{aligned}\
\\\\
x_i  \ge 0,i = 1,2,...,r
\end{split}</script><p>的变量 $x_1,x_2,…,x_r$的值。</p>
<p><strong>最优可行解</strong>：使目标函数 $y=\sum \limits_{i=1}^r c_ix_i$ 取到最大值（或最小值）的可行解</p>
<p><strong>可行解域：</strong>所有可行解的集合</p>
<h4 id="（2）例子"><a href="#（2）例子" class="headerlink" title="（2）例子"></a>（2）例子</h4><p><strong>例 1</strong>：给出一个线性规划问题如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = x_1 + x_2 \\\\
\begin{aligned}
s.t. 2x_1+3x_2 & \le 6 \\\\
3x_1+2x_2 & \le 6 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>则画图如下：</p>
<img src="/2022/03/19/LinearProgramming/image-20220319171444450.png" class="" title="例1-图1">
<img src="/2022/03/19/LinearProgramming/image-20220319171538409.png" class="" title="例1-图2">
<p>如图所示，在点 (6/5,6/5) ， $y$ 取得最大值 12/5 ，即最优可行解为 (6/5,6/5) 。</p>
<p><strong>例 2</strong> 给出一个线性规划问题如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = 3x_1 + x_2 \\\\
\begin{aligned}
s.t. x_1+x_2 & \le 5 \\\\
-x_1+x_2 & \le 0 \\\\
6x_1+2x_2 & \le 21 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>则画图如下：</p>
<img src="/2022/03/19/LinearProgramming/image-20220318114944778.png" class="" title="image-20220318114944778">
<img src="/2022/03/19/LinearProgramming/image-20220318115111885.png" class="" title="image-20220318115111885">
<p>如图所示，$y$ 可取得最大值 21/2 ，但有无穷多个最优可行解。</p>
<p><strong>例 3</strong> 给出一个线性规划问题如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = 2x_1 + 2x_2\\\\
\begin{aligned}
s.t. x_1 - x_2 & \ge 1 \\\\
-x_1+2x_2 & \le 0 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>则画图如下：</p>
<img src="/2022/03/19/LinearProgramming/image-20220318115306288.png" class="" title="image-20220318115306288">
<p>如图所示，可行解域无穷大，$y$ 无法取到最大值， 有无穷多个最优可行解。</p>
<p><strong>例 4</strong> 给出一个线性规划问题如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{min} \ y = 3x_1 + 2x_2 \\\\
\begin{aligned}
s.t. -x_1 + x_2 & \ge 1 \\\\
x_1+x_2 & \le -2 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>则画图如下：</p>
<img src="/2022/03/19/LinearProgramming/image-20220318115526464.png" class="" title="image-20220318115526464">
<p>如图所示，没有可行解，因此 $y$ 无法取到最大值，没有最优可行解。</p>
<h4 id="（3）解的-4-种情况"><a href="#（3）解的-4-种情况" class="headerlink" title="（3）解的 4 种情况"></a>（3）解的 4 种情况</h4><p><strong>两个变量的线性规划问题的解可能有以下四种情况：</strong></p>
<p>1.有唯一的最优可行解。这个唯一的最优可行解一定是可行解域的一个顶点。</p>
<p>2.有最优可行解，但不唯一。此时最优可行解一定是可行解域的一条边界上的所有点。</p>
<p>3.有可行解，但没有最优可行解。此时在可行解域上，目标函数的值趋向无穷。</p>
<p>4.无可行解。此时可行解域为空集。</p>
<h3 id="3-凸多边形与凸多面体"><a href="#3-凸多边形与凸多面体" class="headerlink" title="3 凸多边形与凸多面体"></a>3 凸多边形与凸多面体</h3><h4 id="（1）相关概念-1"><a href="#（1）相关概念-1" class="headerlink" title="（1）相关概念"></a>（1）相关概念</h4><p><strong>凸多边形：</strong>是指没有一个内角超过 180。的多边形。或者说，一个多边形是凸的，如果联接这个多边形中任意两个点的线段上的所有点仍在这个多边形中.</p>
<p><strong>凸多边形的角点 x：</strong>它在凸多边形中且不在凸多边形中任何两点（除 x 点外）的线段上。</p>
<p>具有两个变量的线性规划问题，其最优可行解如果存在，一定在角点上可以找到最优可行解。</p>
<p>推广到多维情形：</p>
<p><strong>凸多面体：</strong>如果联接这个多面体中任意两个点的线段上的所有点仍在这个多面体中。</p>
<h4 id="（2）-r-维空间中的点"><a href="#（2）-r-维空间中的点" class="headerlink" title="（2）$r$ 维空间中的点"></a>（2）$r$ 维空间中的点</h4><p>在 $r$ 维空间中，一个点可以用这个点的 $r$ 个坐标来表示.</p>
<script type="math/tex; mode=display">
\begin{split}
a_{i1}x_1+a_{i2}x_2+...+a_{ir}x_r=bi\\\\
a_{i1}x_1+a_{i2}x_2+...+a_{ir}x_r>bi\\\\
a_{i1}x_1+a_{i2}x_2+...+a_{ir}x_r<bi
\end{split}</script><h4 id="（3）线性规划问题的几何表示"><a href="#（3）线性规划问题的几何表示" class="headerlink" title="（3）线性规划问题的几何表示"></a>（3）线性规划问题的几何表示</h4><p>对一个有 $r$ 个变元和 $m$ 个线性约束条件的线性规划问题，可行解域是 $r$ 维空间中的一个凸多面体，它由对应于线性约束条件的 $m$ 个超平面和对应于非负条件的 $r$ 个超平面所围成：</p>
<script type="math/tex; mode=display">
\begin{split}
a_{11}x_1+a_{12}x_2+...+a_{1r}x_r=b_1\\\\
a_{21}x_1+a_{22}x_2+...+a_{2r}x_r=b_2\\\\
...\\\\
a_{m1}x_1+a_{m2}x_2+...+a_{mr}x_r=b_m\\\\
x_i \ge 0,i = 1,2,...,r
\end{split}</script><p>对 $r$ 维空间的线性规划问题，如果存在最优可行解，那么在凸多面体的某个角点上一定能找到最优可行解。</p>
<p>在二维空间中，互不平行的两条直线相交于一点，在 $r$ 维空间中，互不平行的 $r$ 个超平面也相交于一个点。</p>
<h3 id="4-线性规划问题的标准形式"><a href="#4-线性规划问题的标准形式" class="headerlink" title="4 线性规划问题的标准形式"></a>4 线性规划问题的标准形式</h3><h4 id="（1）标准形式"><a href="#（1）标准形式" class="headerlink" title="（1）标准形式"></a>（1）标准形式</h4><p>给定一个线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max(min)} \ y = \sum  \limits_{i=1}^r c_ix_i \\\\
\begin{aligned}
s.t. \quad \sum \limits_{i=1}^r a_{1i}x_i & \le(=,\ge) b_1 \\\\
 \sum \limits_{i=1}^r a_{2i}x_i & \le(=,\ge) b_2 \\\\
 ......\\\\
  \sum \limits_{i=1}^r a_{2mi}x_i & \le(=,\ge) b_m \\\\
\end{aligned}\\\\
\quad x_i   \ge 0,i  = 1,2,...,r
\end{split}
\end{equation}</script><p>将其标准化，为</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y  = \sum  \limits_{i=1}^r c_ix_i \\\\
\begin{aligned}
s.t.   \sum \limits_{i=1}^r a_{1i}x_i  &= b_1 \\\\
 \sum \limits_{i=1}^r a_{2i}x_i  &= b_2 \\\\
 ......\\\\
  \sum \limits_{i=1}^r a_{2mi}x_i  &= b_m \\\\
\end{aligned} \\\\
x_i   \ge 0,i  = 1,2,...,r
\end{split}
\end{equation}</script><h4 id="（2）标准化方法"><a href="#（2）标准化方法" class="headerlink" title="（2）标准化方法"></a>（2）标准化方法</h4><p>（a）如果约束条件中出现了 $&lt;,\le ,&gt;,\ge$ ，则需要引入松弛变量：</p>
<script type="math/tex; mode=display">
\begin{align}
a_{k1}x_1+a_{k2}x_2 +...+a_{kr}x_r \le b_k \\\\
\text{插入}{x_{r+k}\ge0}\Rightarrow a_{k1}x_1+a_{k2}x_2 +...+a_{kr}x_r +x_{r+k}= b_k
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
a_{k1}x_1+a_{k2}x_2 +...+a_{kr}x_r \ge b_k \\\\
\text{插入}{x_{r+k}\ge0}\Rightarrow a_{k1}x_1+a_{k2}x_2 +...+a_{kr}x_r -x_{r+k}= b_k
\end{align}</script><p>（b）如果线性规划问题求最小值，那么我们可以将等式左右取负转为求最大值问题：</p>
<script type="math/tex; mode=display">
\text{min} \ y = \sum  \limits_{i=1}^r c_ix_i \Rightarrow \text{max} \ y' = -y = -\sum  \limits_{i=1}^r c_ix_i</script><p>（c）如果约束条件中的 $b_k &lt;0$ ，那么我们可以将等式左右取负将等式后边置为正值：</p>
<script type="math/tex; mode=display">
a_{k1}x_1+a_{k2}x_2 +...+a_{kr}x_r = b_k \Rightarrow\text{插入}{b_k < 0}\Rightarrow -a_{k1}x_1-a_{k2}x_2 -...-a_{kr}x_r = -b_k</script><p>（d）如果某个变量 $x_i &lt; 0$ ，做如下变换：</p>
<script type="math/tex; mode=display">
x_i = x_i'-x_i''(x_i'' > x_i' \ge 0)</script><p>（e）这样我们就可以得到标准化的线性规划问题：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = \sum  \limits_{i=1}^r c_ix_i \\\\
\begin{aligned}
s.t. \sum \limits_{i=1}^r a_{1i}x_i \pm x_{r+1} & = b_1 \\\\
\sum \limits_{i=1}^r a_{2i}x_i \pm x_{r+2} & = b_2 \\\\
 ......\\\\
\sum \limits_{i=1}^r a_{2mi}x_i \pm x_{r+m}  & = b_m
\end{aligned}
 \\\\
 x_i   \ge 0,x_{r+j} \ge 0,i  = 1,2,...,r \\\\
 b_i  \ge 0,i=1,2,...,m
\end{split}
\end{equation}</script><p>观察得到，$(x<em>1,x_2,…,x_r,x</em>{r+1},x<em>{r+2},…x</em>{r+m})$ = $(0,0,…,0,b_1,b_2,…,b_m)$ 是当前约束条件下</p>
<p>的一个可行解。</p>
<h3 id="5-线性规划问题的基本定理"><a href="#5-线性规划问题的基本定理" class="headerlink" title="5 线性规划问题的基本定理"></a>5 线性规划问题的基本定理</h3><h4 id="（1）基本可行解的相关概念"><a href="#（1）基本可行解的相关概念" class="headerlink" title="（1）基本可行解的相关概念"></a>（1）<strong>基本可行解的相关概念</strong></h4><p><strong>基本可行解：</strong>一个可行解的 $r+m$ 个分量中的 $r$ 个分量都是 0 称为基本可行解；如果有多于 $r$ 个 0 的分量，则为退化的基本可行解。</p>
<p>基本可行解中，取非 0 值的变量称为<strong>基本变量</strong>，其他的称为<strong>非基本变量</strong>。</p>
<p><strong>最优基本可行解：</strong>使目标函数达到最优的基本可行解。</p>
<h4 id="（2）最优基本可行解存在定理"><a href="#（2）最优基本可行解存在定理" class="headerlink" title="（2）最优基本可行解存在定理"></a>（2）<strong>最优基本可行解存在定理</strong></h4><p><strong>最优基本可行解存在定理(线性规划问题基本定理）：</strong>如果线性规划问题存在最优可行解，则一定存在最优基本可行解。</p>
<p>这个定理的证明是构造性的，我们直接在后面的求解方法中介绍。</p>
<p><strong>推论：线性规划问题的最优可行解若存在，则必在可行解域的角点上找到。</strong></p>
<p>只需在可行解域的有限个角点处计算目标函数的值，然后将这些值进行比较，从中挑出最优的值。</p>
<p>当变量的数目和线性约束条件的数目增加时，可行解域中的角点就会大大增加，从而在每一角点处计算目标函数的值就会花费许多工作量。</p>
<h3 id="6-单纯形方法"><a href="#6-单纯形方法" class="headerlink" title="6 单纯形方法"></a>6 单纯形方法</h3><h4 id="（1）基本思想"><a href="#（1）基本思想" class="headerlink" title="（1）基本思想"></a>（1）基本思想</h4><img src="/2022/03/19/LinearProgramming/image-20220318140958990.png" class="" title="image-20220318140958990">
<h4 id="（2）存在问题"><a href="#（2）存在问题" class="headerlink" title="（2）存在问题"></a>（2）存在问题</h4><ul>
<li>计算了目标函数在某一角点处的值之后，如何求得另一角点使得目标函数在该角点的值会更大些？</li>
<li>当可行解域的相应角点已经达到时，如何知道已经求得最优解？</li>
</ul>
<h4 id="（3）例子"><a href="#（3）例子" class="headerlink" title="（3）例子"></a>（3）例子</h4><script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = x_1 + 4x_2 \\\\
\begin{aligned}
s.t. 4x_1 + 5x_2 & \le 10 \\\\
5x_1 + 2x_2 & \le 10\\\\
-7x_1+4x_2 & \le 4 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>标准化后，为：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\text{max} \ y = x_1 + 4x_2 \\\\
\begin{aligned}
s.t. 4x_1 + 5x_2 + x_3 & = 10 \\\\
5x_1 + 2x_2 + x_4 & = 10 \\\\
-7x_1+4x_2 +x_5 & = 4 \\\\
x_1  & \ge 0 \\\\ x_2 & \ge 0\\\\
\end{aligned}
\end{split}
\end{equation}</script><p>注意这里 $r=2,m=3$</p>
<img src="/2022/03/19/LinearProgramming/image-20220318143957406.png" class="" title="image-20220318143957406">
<p>在 $m$ 个约束条件中，令 $r+m$ 个变量的 $r$ 个(非基本变量)为 0，解出其余 $m$ 个变量，就可以得到一个基本可行解，对应一个角点。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/14/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BD%91%E7%BB%9C%E6%B5%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Rebecca">
      <meta itemprop="description" content="弱小和无知不是生存的障碍，傲慢才是。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rebecca的赛博世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/14/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BD%91%E7%BB%9C%E6%B5%81/" class="post-title-link" itemprop="url">算法之网络流</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-14 23:24:24 / 修改时间：23:58:39" itemprop="dateCreated datePublished" datetime="2022-03-14T23:24:24+08:00">2022-03-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本章大纲：</p>
<ol>
<li>理解最大流、任意流、最小割、任意割之间的关系；</li>
<li>掌握网络最大流问题和最小割问题及其求解算法，给出一个网络能求出它的最大流或者最小割。</li>
</ol>
<h2 id="最大流和最小割问题"><a href="#最大流和最小割问题" class="headerlink" title="最大流和最小割问题"></a>最大流和最小割问题</h2><ol>
<li><p>流网络</p>
<ol>
<li>一个流网络是一个图 $G=(V,E,s,t,c)$<ol>
<li>$(V,E)$ 为一个有向图，起点为 $s\in V$ ，终点为 $t \in V$</li>
<li>对每一条边 $e\in E$ ，容量 $c(e) &gt; 0$</li>
</ol>
</li>
</ol>
</li>
<li><p>最大流问题</p>
<ol>
<li>一个流网络 $f$ 是一个函数，满足<ol>
<li>对于每一条边 $e \in E$ ： $0 \le f(e)\le c(e)$ </li>
<li>对于每一个结点 $v \in V-\lbrace s, t \rbrace$ ： $\sum<em>{e \in \ to \ v } f(e)=\sum</em>{e \ out \ of \ v}$</li>
</ol>
</li>
<li>流值 $val(f)=\sum<em>{e \ out \ of \ s}  f(e) - \sum</em>{e \ in \  to \  s}$</li>
<li>最大流问题：找到一个流的最大流值。</li>
</ol>
</li>
<li><p>最小割问题</p>
<ol>
<li><p>一个割集是一个划分 $(A,B)$ ，满足 $s \in A$ 和 $t\in B$</p>
</li>
<li><p>割集的容量是从 $A$ 到 $B$​ 的所有边的容量之和</p>
<script type="math/tex; mode=display">
cap(A,B)=\sum_{e \ out \ of \ A}c(e)</script></li>
<li><p>最小割问题：找到一个割的最小容量</p>
</li>
</ol>
</li>
</ol>
<h2 id="Ford–Fulkerson算法"><a href="#Ford–Fulkerson算法" class="headerlink" title="Ford–Fulkerson算法"></a>Ford–Fulkerson算法</h2><ol>
<li><p>贪心算法</p>
<ol>
<li>对每一条边  $e \in E$ ，最开始初始化 $f(e)=0$ ，即每一条边上的流量为0</li>
<li>找到从起点 $s$ 到终点 $t$ 的路径 $P$ ，满足 $f(e)&lt;c(e)$ ，即找到从起点到终点流量小于容量的路径</li>
<li>将 $P$ 作为增广路径进行增广</li>
<li>一直重复知道算法终止</li>
</ol>
</li>
<li><p>贪心算法不能得到最优解</p>
<ol>
<li>贪心算法无法撤销那些“坏”的操作</li>
</ol>
</li>
<li><p>残量网络</p>
<ol>
<li><p>原来的边 $e\ = (u,v )\in E$</p>
<ol>
<li>边上的流为 $f(e)$</li>
<li>边上的容量为 $c(e)$</li>
</ol>
</li>
<li><p>反向边 $e^{reverse}=(v,u)$</p>
<ol>
<li>反向边就是对流的撤销操作</li>
</ol>
</li>
<li><p>残留容量</p>
<script type="math/tex; mode=display">
c_f(e)=
\begin{cases}
c(e)-f(e) & if\ e \in E\\ 
f(e) & if\ e^{reverse}\in E 
\end{cases}</script></li>
<li><p>残量网络 $G_f=(V,E_f,s,t,c_f)$</p>
<ol>
<li>$E_f=\lbrace e:f(e)<c(e)\rbrace \cup \lbrace e^{reverse}:f(e)>0\rbrace$ ，即残量网络的边集是由流量小于容量的边以及那些有流量的反向边的并集。</li>
<li>关键性质：$f’$ 是一个在 $G_f$ 中的流，当且仅当 $f+f’$ 是 $G$  上的流。</li>
</ol>
</li>
</ol>
</li>
<li><p>在残量网络中运行贪心算法即为Ford–Fulkerson算法</p>
<ol>
<li><p>增广路径是在 $G_f$ 上的一条由 $s\rightarrow t$ 的简单路径</p>
</li>
<li><p>瓶颈容量是增广路径 $P$ 上的所有边的最小残留容量。</p>
</li>
<li><p>关键性质：令 $f$ 为一个流，$P$ 为 $G_f$ 上的一条增广路径。在调用 $f’\leftarrow AUGMENT(f,c,P) $ 的算法之后，结果 $f’$ 是一个流，同时，$val(f’)=val(f)+bottleneck(G_f,P)$ </p>
</li>
<li><p>增加增广路径的伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">AUGMENT(f,c,P)</span><br><span class="line"></span><br><span class="line">δ ← 增广路径P的瓶颈容量</span><br><span class="line">FOREACH P中的边e</span><br><span class="line">	IF (e ∈ E) </span><br><span class="line">		f(e) ← f(e) + δ</span><br><span class="line">	ELSE </span><br><span class="line">		f(e_r) ← f(e_r) - δ</span><br><span class="line">RETURN f</span><br></pre></td></tr></table></figure>
<p>注：伪代码中的 e_r 即为 $e^{reverse}$</p>
</li>
<li><p>Ford–Fulkerson算法</p>
<ol>
<li>对每一条边  $e \in E$ ，最开始初始化 $f(e)=0$ ，即每一条边上的流量为0</li>
<li>在 $G_f$ 中找到从起点 $s$ 到终点 $t$ 的路径 $P$ ，满足 $f(e)&lt;c(e)$ ，即找到从起点到终点流量小于容量的路径</li>
<li>将 $P$ 作为增广路径进行增广</li>
<li>一直重复直到算法终止</li>
</ol>
</li>
<li><p>Ford–Fulkerson算法伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Ford–Fulkerson(G)</span><br><span class="line"></span><br><span class="line">FOREACH e∈E</span><br><span class="line">	f(e) ← 0</span><br><span class="line">Gf ← G的残量网络</span><br><span class="line">WHILE (在Gf中存在一条s→t的路径P)</span><br><span class="line">	f ← AUGMENT(f,c,P)</span><br><span class="line">	更新Gf</span><br><span class="line">RETURN f</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<ol>
<li><p>正确性证明</p>
</li>
<li><p>运行时间</p>
<ol>
<li>假设每一条边上的容量 $c(e)$ 是一个介于 $1$ 和 $C$ 之间的整数</li>
<li>经过Ford–Fulkerson算法，每一条边上的流值 $f(e)$ 和残留容量 $c_f(e)$ 是一个整数</li>
<li>Ford–Fulkerson算法在最多 $ n C$ 条增广路径后停止，且 $val(f^<em>) \le n C$ ，其中 $f^</em>$ 为最大流</li>
<li>Ford–Fulkerson算法的运行时间为 $O(mnC)$ ，因为使用BFS或DFS寻找增广路径使用$O(m)$ 的时间。</li>
<li>整数定理：存在一个整数的最大流 $f^*$</li>
</ol>
</li>
</ol>
<h2 id="最大流最小割定理"><a href="#最大流最小割定理" class="headerlink" title="最大流最小割定理"></a>最大流最小割定理</h2><ol>
<li><p>流值引理</p>
<ol>
<li>令 $f$ 为任意流，$(A,B)$ 为任意割集，那么，流 $f$ 的值 $val(f)$ 等于经过割集 $(A,B)$ 的流</li>
</ol>
</li>
<li><p>流和割构成弱对偶关系</p>
<ol>
<li><p>令  $f$ 为任意流，$(A,B)$ 为任意割集，那么，流 $f$ 的值 $val(f)$ 小于割集 $(A,B)$ 的容量。</p>
</li>
<li><p>证明：</p>
<script type="math/tex; mode=display">
\begin{align}
val(f) &=\sum_{\text{e out of A}}{f(e)} - \sum_{\text{e in to A}}{f(e)}
\\ &\le \sum_{\text{e out of A}}{f(e)}
\\ &\le \sum_{\text{e out of A}}{c(e)}
\\ &= cap(A,B)
\end{align}</script></li>
</ol>
</li>
<li><p>最优性条件</p>
<ol>
<li>令  $f$ 为任意流，$(A,B)$ 为任意割集，那么，当流 $f$ 的值 $val(f)$ 等于割集 $(A,B)$ 的容量时，流 $f$ 为最大流，割集 $(A,B)$ 为最小割。</li>
</ol>
</li>
<li><p>最大流和最小割定理</p>
<ol>
<li><p>最大流的流值等于最小割的容量</p>
<ol>
<li>我们通过一下三个条件来进行命题的证明。<ol>
<li>存在一个割集 (A,B)，$cap(A,B)=val(f)$</li>
<li>$f$ 是一个最大流</li>
<li>$f$ 没有增广路径</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="变尺度算法"><a href="#变尺度算法" class="headerlink" title="变尺度算法"></a>变尺度算法</h2><ol>
<li><p>如何选择增广路径</p>
</li>
<li><p>变尺度算法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CAPACITY-SCALING(G)</span><br><span class="line"></span><br><span class="line">FOREACH e∈E</span><br><span class="line">	f(e) ← 0</span><br><span class="line">Δ ← 2的最大幂次（&lt;= C)</span><br><span class="line">WHILE (Δ &gt;= 1)</span><br><span class="line">	Gf(Δ) ← G关于流f的Δ-残量网络</span><br><span class="line">	WHILE(在Gf(Δ)中存在一条s→t的路径P)</span><br><span class="line">		f ← AUGMENT(f,c,P)</span><br><span class="line">		更新Gf(Δ)</span><br><span class="line">	Δ ← Δ/2</span><br><span class="line">RETURN f</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>正确性证明</p>
<ol>
<li>假设，所有边的容量都是介于1到C的整数</li>
<li>变尺度参数 $\Delta$ 是一个2的幂次</li>
<li>$\Delta=1$ 时，没有增广路径</li>
</ol>
</li>
<li><p>运行时间</p>
<ol>
<li><p>引理1：算法最多循环 $1+\left \lfloor  logC\right \rfloor$ 次</p>
<ol>
<li>最开始初始化时，$C/2 &lt; \Delta \le C$ ，而 $\Delta$ 在每次循环中会除以 $2$ .</li>
</ol>
</li>
<li><p>引理2：令 $f$ 为一个 $\Delta$-变尺度阶段后的流，则最大流值不超过 $val(f)+m\Delta$</p>
<ol>
<li><script type="math/tex; mode=display">
\begin{align}
val(f) &=\sum_{\text{e out of A}}{f(e)} - \sum_{\text{e in to A}}{f(e)}
\\&\ge\sum_{\text{e out of A}}{(c(e)-\Delta)} - \sum_{\text{e in to A}}{\Delta}
\\&\ge \sum_{\text{e out of A}}c(e) - \sum_{\text{e out of A}}{\Delta} -\sum_{\text{e in to A}}{\Delta}
\\&\ge cap(A,B)-m\Delta
\end{align}</script></li>
<li><p>流出边的流量一定$\le \Delta$ ，流入边的容量一定 $\le \Delta$</p>
</li>
</ol>
</li>
<li><p>引理3：每个阶段最多进行 $2m$ 次增广</p>
<ol>
<li>令 $f$ 为一个 $\Delta$-变尺度阶段<strong>前</strong>的流</li>
<li>由引理2可知，最大流值不超过 $val(f)+m (2\Delta)$</li>
<li>而在$\Delta$阶段，每次增广至少$\Delta$的流量。</li>
</ol>
</li>
<li><p>定理：变容算法运行时间为 $O(m^2logC)$</p>
</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rebecca"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Rebecca</p>
  <div class="site-description" itemprop="description">弱小和无知不是生存的障碍，傲慢才是。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022-03 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rebecca</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
